---
title: 上下文工程(Context Engineering)
date: 2025-10-11
tags:
  - 调研
  - ContextEngineering
  - LLM
categories: 果冻的理论学习
cover: ./img/logoZiyouzhiyi.jpg
---
## **前言**

随着大模型能力的快速发展，人们对大语言模型的使用方式越来越多的从简单的ChatBot，变成了各种Agent，而一个新的术语——上下文工程（Context Engineering），也逐步取代了[[提示工程]](Prompt Engineering）。

最近[Anthropic]发表了一篇文章，分享了他们在上下文工程上的探索，本文基于原文做了适当解读。

## **上下文工程和提示工程的区别---记忆**

简单来说，提示工程重点关注的是为了获得最佳结果而编写和组织的LLM指令的方法，各种在公众号、B站、小红书上所介绍的如何在某一款软件中输入特定的指令从而能够获得某种效果的方法，都属于提示工程，它通常涉及需要完成的任务、任务的背景信息、如何更好的完成任务、哪些是禁止的、受众是谁等，例如如何使用AI写文章、如何去除AI味、如何使用豆包生成特定的图片等。

而上下文工程指的是，在LLM推理过程中，如何管理和维护要输入给LLM的**最佳信息**的**策略集**，之所以叫信息，而没有叫Prompt或者提示语，是因为在上下文工程中，除了涉及给LLM安排任务，还涉及到工具、当前任务所需要的参考资料、之前交互过程中的记忆等，参考资料和记忆其实也看做是背景信息，但与提示工程中背景信息的区别是他们通常是随任务动态变化的。

下图是Anthropic给出的提示工程和上下文工程的对比图：

![](https://pic4.zhimg.com/v2-91ac54ea44495506424e26e1a17b6b83_1440w.jpg)

对提示工程而言，使用编写好的提示语（Prompt）通常在一个轮次就可以得到结果，而上下文工程是迭代性的，它通常需要根据LLM的响应动态组织上下文，大家使用Manus、扣子空间之类的这些产品时，看它很忙碌的样子，背后就经历了多个轮次的上下文调整，而这些上下文该以什么方式进行调整，背后对应的就是上下文工程。

## **有效上下文应该是什么结构**

简而言之，有效的上下文应该是不长不短，恰到好处。

Anthropic建议将提示语（这里更多的是指系统提示语System Prompt）组织成不同的部分，比如 `<background_information>` 、 `<instructions>` 、 `## Tool guidance` 、 `## Output description` 等，并使用XML 标签或 Markdown 标题等技术来区分这些部分。

无论怎么构建系统提示语，都应该完整地概述你的预期行为，这是最核心的信息。

![](https://pic1.zhimg.com/v2-41c9e96a7552051ae1a93ee5b49c9936_1440w.jpg)

Anthropic在实践中观察到，构建Agent最常见的失败模式之一是工具集冗余，导致模型在工具选择上出现困难。**如果人类工程师在给定相关信息后无法判断该什么工具，那也别指望人工智能做得有多好。**

提示样例，或者小样本提示（Few Shot），是一种众所周知的最佳实践，联想上面提到的过于复杂、脆弱的逻辑，开发者倾向于往提示中加入各种边缘判断情况，试图把LLM在特定情况下应遵循的所有规则都加入进去，Anthropic不建议这样做，建议使用提示样例。

## **如何构造有效的上下文**

这里的核心，是需要对上下文做动态检索。

Claude Code的实践经验就是，将所有需要的数据进行预先处理，**模型上下文中只保留这些处理结果的标识符（例如文件路径、存储插叙、网页链接等），让智能体采用按需取用的方式，通过工具动态将数据加载到上下文中**。

这样会使上下文的使用效率很高，同时，**放入上下文的引用元数据，还提供了额外的信息，例如文件夹的层级结果、文件名、时间戳等，它能帮助人类和Agent理解何时该如何使用信息。**

让Agent自主探索和检索数据，还能实现额外的好处，例如文件大小通常暗示了文件的复杂性（越大的文件越复杂）、命名规范暗示了文件的用途（例如test.py结尾的文件可能是测试文件），这些信息，都可以让Agent在运行时自主探索，逐渐理解，仅在上下文中保留必要的信息，避免上下文被大量可能无关的信息淹没。

当然这种动态导航和探索的方式也不是没有缺点，最主要的一点就是，它会比直接加载预计算好的信息慢。

Anthropic的实践是采用混合策略，像CLAUDE.md（里面一般会有开发需要用到的配置信息、开发规范等）这样的文件，会直接加载入上下文，而其他信息，则会在需要的时候，通过glob、grep等工具动态检索。

> 混合策略算是效率和效果的一种平衡，可以理解成高频要用的信息，直接加载入上下文，不要再浪费时间探索了，低频信息采用动态检索的方式，这跟计算机压缩算法中，高频字符用短编码，低频信息用长编码的做法真是有异曲同工之妙。

## **压缩**

**压缩是指对接近模型最大上下文长度时对对话进行内容总结，并重新初始化一个新的上下文窗口**。压缩通常作为上下文工程的第一种手段，以提升长期连贯性。其核心在于以高保真方式提炼上下文窗口的内容，使智能体能够以最小的性能下降继续执行。

像在Claude Code中，就会在上下文长度达到模型最大上下文长度的92%时自动压缩，通过将消息历史传递给模型来实现这一点，以总结和压缩最关键的信息。模型保留了架构决策、未解决的错误和实现细节，同时丢弃了冗余的工具输出或消息。

压缩的艺术在于选择保留什么与丢弃什么，因为过于激进的压缩可能导致后来才显现重要性的关键的上下文信息丢失。对于实施压缩系统的工程师，建议在复杂的Agent构建时仔细调整Prompt。调整Prompt时，首先从确保压缩后能最大化召回所有相关信息开始，然用开始迭代，通过逐步删除冗余内容的方式来提高精确度。

> 这个方法挺值得借鉴的，因为很难一次性写出一个完美的压缩Prompt，甚至你可能都不知道该怎么评价这个Prompt的好坏，这里给出的方法就是把目标分解成先确保信息少丢失（最大化召回），再权衡（还要精准），给出了一个清晰可操作的流程。 有点像分类模型调整分类阈值，先保Recall，然后开始提高分类阈值，让Precision升高、Recall下降别太多，来寻找一个最佳决策点。

一个可被压缩的地方是，清理工具调用和调用结果，为什么？因为调用工具通常是为了使用工具调用结果，一般大模型都会根据System Prompt中的指令对工具调用结果做处理，既然已经有了模型处理结果，那原始的工具调用、工具调用结果信息就不再需要了，可以直接清除。

## **结构化笔记记录**

结构化笔记记录，或代理式记忆，是一种技术，其中代理定期将笔记保存在上下文窗口之外的存储（例如硬盘）中，这些笔记在稍后会被拉回上下文窗口。

把大模型的上下文窗口想象成电脑的内存，把上下文窗口之外的存储想象成硬盘，就好理解了，跟电脑内存不足，把内存中的页先交换到硬盘的真是一样一样的。

像有些介绍Cursor、Roo Code、Claude Code的教程，一开始先让Agent生成需求文档、设计文档和待办事项，并写入硬盘的做法，都属于这一类，在后续某个步骤需要这些内容时，再加载回上下文。

此处不知道大家有没有疑问，把原来一整段的内容先写入硬盘，需要的时候再读取回来放入上下文，上下文窗口不一样会变长吗，跟直接放到上下文中有啥区别？核心是Agent一般都会有文件读写的工具，而文件读取后，在大模型消化完这个信息后，读取的内容就可以丢弃了（回想一下上面提到的清理工具调用结果的部分），举个例子就好理解了，比如代码智能体前期做好了系统设计，并将设计文档写入了硬盘，等开始实现用户管理模块时，它只需要先检索到这部分内容（或者粗暴点，直接整个设计文档加载入上下文），那么当智能体将用户管理模块编码完成后，这个设计文档的内容就可以移出上下文了，所以这种按需取用的方式，可以显著降低上下文占用。

## **子智能体架构**

原文中所说的这种子智能体架构模式（Sub-agent architectures），是多智能体架构中常见的一种设计方式，它通常由主智能体（Orchestrator）、子智能体组成。由主智能体分拆协调任务、子智能体处理特定任务。

这种架构最核心的作用是**上下文隔离**，这种方式从某种角度上看其实是扩大了模型所能支持的最大上下文长度，相当于把之前巨长、可能已经超过模型最大输入长度的上下文，分成了不同的部分，每个部分都不会超长，同时缓解了信息过载对模型造成的认知负担。

通过给每个智能体组织不同的上下文，让它集中所有能力解决它的问题，从而可以带来比单智能体更好的效果。

当然它的实现也是有挑战的，否则现在所有的智能体架构都是多智能体架构了，比如有如下几点：

- **任务拆分**：主智能体在向各子智能体安排任务时，如何做到不重不漏，任务依赖关系合理，重复以外这更长的耗时和更多的token消耗，遗漏意味着最终任务可能无法完成，依赖关系不合理意味着某个智能体可能会存在信息缺失风险
- **上下文传递**：主智能体安排了若干子智能体完成不同的任务，如何很好地将子智能体的结果传递给别的子智能体，比如编写一份调研报告时，报告撰写智能体已经完成了初版报告，审阅智能体看到后提出意见“调研报告不够全面，为什么报告中有中国市场、日韩市场、欧洲市场、东南亚市场相关的调研结果，没有北美市场的”，它不知道的是，报告撰写智能体其实调研了北美市场，只是没有找到相关信息
- **结果整合**：任务拆解让各子任务完成时，肯定是希望最终解决用户原始问题的，但各子智能体的处理结果，很有可能互相之间是没有关联的，比如目标是开发一个FlappyBird游戏，一个子智能体负责生成能够上下移动不断前进的小鸟，一个负责生成有一些烟囱的草坪，当这两者整合时，发现生成草坪的智能体，生成的是类似超级马里奥那样的草坪，两者根本无法整合（案例来自[Cognition | Don’t Build Multi-Agents](https://link.zhihu.com/?target=https%3A//cognition.ai/blog/dont-build-multi-agents)）