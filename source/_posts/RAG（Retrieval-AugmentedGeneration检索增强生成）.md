---
title: RAG（Retrieval-AugmentedGeneration检索增强生成）
date: 2025-10-01
tags:
  - LLM
  - tools
  - RAG
categories: 果冻的理论学习
cover: ./img/logoZiyouzhiyi.jpg
---
## 写在前面

RAG 是一种新兴的 AI 技术，它结合了信息检索和生成式 AI 的优势，能够在处理复杂任务时提供更准确和相关的答案。RAG 的实现过程相对简单，但是它的核心在于如何检索到相关的信息，以及如何将这些信息与 LLM 结合，这就是 RAG 的关键所在。

## LLM的局限性

RAG（Retrieval-Augmented Generation 检索增强生成）正如其字面意思，是通过检索信息来增强 LLM生成的能力。RAG 是一种新兴的 AI 技术，它结合了信息检索和生成式 AI 的优势，能够在处理复杂任务时提供更准确和相关的答案。

RAG 的工作原理是：首先通过检索引擎从一个大型知识库中获取相关信息，然后将这些信息通过 Prompt 工程与 LLM 结合，生成最终的答案。

为什么需要这样做呢？ LLM的特点：

- 会回答你很多不存在的东西，编造一些不存在的事实，或者是对事实进行错误的推理，LLM 回答不存在的东西的现象被称为幻觉（Hallucination）。
- 尤其不擅长于最新的技术和最近的热点事件（注：这里排除掉能够使用联网能力的产品），因为 LLM 训练时使用的数据集是有截止日期的。
## RAG实现

流程是: **用户输入问题-检索相关信息-将信息与LLM结合-生成最后答案**

实现就是这么简单，但是它的核心在于如何检索到相关的信息，以及如何将这些信息与 LLM 结合，这就是 RAG 的关键所在. 对于一个RAG 产品来说，检索的质量和 LLM 的生成能力是两个最重要的指标，它们直接决定了给用户呈现的最终效果，而对 AI 落地产品来说，检索的质量更是重中之重，因为 AI 落地产品的核心就是要解决用户的问题，而不是让用户来解决 AI 的问题。

那么，如何提高检索的质量呢？

模糊搜索/关键词匹配？那肯定不行，用户输入的问题可能是抽象的，常规的模糊搜索算法可能在用户一长段问题中找不到任何相关的内容，或者是找到的内容和用户的问题完全不相关，自然语言暗藏了太多的语义信息，模糊搜索和关键词匹配都无法捕捉到这些信息。

大语言模型的核心就是语言模型，语言模型的核心就是概率分布，RAG 的检索过程其实就是一个概率分布的过程，但是 LLM 是需要经过训练的，训练的过程就是不断的调整模型参数，使得模型能够更好的拟合数据集中的概率分布

### 自然语言检索核心: Embedding

Embedding模型就是用来解决这类自然语言检索问题的，它的核心思想就是将自然语言转换成向量（Vector），然后通过计算向量之间的距离来判断它们之间的相似度。

#### 实例

假设我们有一个问题：“你能给我推荐一个用一个红色水果做品牌名称的科技公司出的产品吗？”

假设我们有一个 Embedding 模型，它能够将这些自然语言转换成抽象高维度向量，向量的每一个维度都代表了一个特征，比如颜色、形状、大小等等。

假设我们有一个知识库，里面有很多关于水果的品牌名称，比如“手机”、“电动汽车”、“智能台灯”，“苹果”、“香蕉”、“西瓜”等等。

假设我们有一个检索算法，它能够计算向量之间的距离，比如欧几里得距离、余弦相似度等。

我们使用余弦相似度来计算向量之间的距离，那么我们可以将问题转换成向量，比如“你能给我推荐一个用一个红色水果做品牌名称的科技公司出的产品吗？”可以转换成一个向量 A，而知识库中的“苹果”，“手机”，“电动汽车”，“智能台灯”等等也可以转换成向量 B，C，D 等等。

对于文本类问题，我们通常使用余弦相似度来计算向量之间的距离，余弦相似度的公式如下：

![[ObsidianPicture/Pasted image 20251001091142.png]]

通过此公式，我们可以计算出向量 A 和 B 等之间的余弦相似度，余弦相似度的值在-1~1之间，值越大表示两个向量越相似。 如果余弦相似度的值大于某个阈值，比如 0.8 ，那么我们就可以认为这两个向量是相似的，也就是问题和知识库中的内容是相关的。

这个时候，我们将检索内容通过 Prompt 工程再喂给 LLM，LLM 就会根据检索到的内容生成最终的答案。

在实际开发中，Embedding 模型(这个也有不同的模型, 效果和性能上有差别)可能是超高维度的，通常在 512 维以上，因此对于传统关系型数据库来说，存储和检索的成本都非常高，因此我们通常会使用一些专门的向量数据库来存储和检索向量，比如 Milvus、Pinecone、Weaviate 等等。

这些向量数据库通常会使用一些高效的索引算法来加速向量的检索，比如 `HNSW`、`IVF`、`PQ` 等等。向量数据库通常已处理好了这些算法，你只需要调用相关 API 即可。

#### Embedding前的分块

在预处理阶段，在Embdding前还有一个文档解析与分块的过程，因为在通常情况下，你所使用的**Embedding 模型能够处理的文本长度是有限制的**，而构建知识库的过程就是将文档解析成小块，然后将这些小块进行向量化，存入向量数据库中.

在分块时，你可以使用不同的算法，你可以固定大小分块，比如说每 512 字符分成一块，但是这样可能会导致语义不连贯，或者是分块过小，导致向量数据库存储的向量数量过多，成本过高。

基于内容意图分块，可以根据内容的语义和上下文进行分块，这样可以更好的保留语义信息，提高检索的质量。

## RAG 分类

上述提到的是 Naive RAG

还有其他的RAG:

- Advanced RAG：它在 Naive RAG 的基础上，增加了一些优化，在检索前、检索中、检索后都增加了一些优化，比如使用多种检索算法进行组合，使用多种 LLM 进行组合，使用多种 Prompt 模板进行组合等等，这样可以提高检索的质量和生成的效果。
    
- Modular RAG：它将 RAG 的各个部分进行模块化，您可以根据自己的需求选择合适的模块进行组合，比如使用不同的检索算法、不同的 LLM、不同的 Prompt 模板等等，这样可以提高 RAG 的灵活性和可扩展性。
    

使用传统的 `Naive RAG` 可能会导致检索的质量和生成的效果不理想，也有可能使用 `Rerank`（重排序）算法来对检索的结果进行重排序，或者使用 `Chain of Thought`（思维链）来对生成的结果进行优化，这些都是 RAG 的变种实现方式。