<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>RLHF（Reinforcement Learning from Human Feedback，基于人类反馈的强化学习） | 果冻小配方</title><meta name="author" content="GuoDong"><meta name="copyright" content="GuoDong"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="RLHF（Reinforcement Learning from Human Feedback，基于人类反馈的强化学习） 是一种用于训练机器学习模型（特别是大型语言模型）的技术方法。它的核心思想是让模型通过与人类的互动反馈来学习和优化自身行为。 主要流程RLHF 通常包含以下三个关键步骤：  监督微调（Supervised Fine-Tuning, SFT）  首先，在高质量的人类标注数据上对预训">
<meta property="og:type" content="article">
<meta property="og:title" content="RLHF（Reinforcement Learning from Human Feedback，基于人类反馈的强化学习）">
<meta property="og:url" content="https://gaoguodong03.github.io/gdBlog/2026/01/02/RLHF%EF%BC%88Reinforcement-Learning-from-Human-Feedback%EF%BC%8C%E5%9F%BA%E4%BA%8E%E4%BA%BA%E7%B1%BB%E5%8F%8D%E9%A6%88%E7%9A%84%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%89/index.html">
<meta property="og:site_name" content="果冻小配方">
<meta property="og:description" content="RLHF（Reinforcement Learning from Human Feedback，基于人类反馈的强化学习） 是一种用于训练机器学习模型（特别是大型语言模型）的技术方法。它的核心思想是让模型通过与人类的互动反馈来学习和优化自身行为。 主要流程RLHF 通常包含以下三个关键步骤：  监督微调（Supervised Fine-Tuning, SFT）  首先，在高质量的人类标注数据上对预训">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://gaoguodong03.github.io/gdBlog/img/touxiang.jpg">
<meta property="article:published_time" content="2026-01-01T16:00:00.000Z">
<meta property="article:modified_time" content="2026-01-02T02:36:16.931Z">
<meta property="article:author" content="GuoDong">
<meta property="article:tag" content="强化学习">
<meta property="article:tag" content="科普">
<meta property="article:tag" content="调研">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://gaoguodong03.github.io/gdBlog/img/touxiang.jpg"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "RLHF（Reinforcement Learning from Human Feedback，基于人类反馈的强化学习）",
  "url": "https://gaoguodong03.github.io/gdBlog/2026/01/02/RLHF%EF%BC%88Reinforcement-Learning-from-Human-Feedback%EF%BC%8C%E5%9F%BA%E4%BA%8E%E4%BA%BA%E7%B1%BB%E5%8F%8D%E9%A6%88%E7%9A%84%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%89/",
  "image": "https://gaoguodong03.github.io/gdBlog/img/touxiang.jpg",
  "datePublished": "2026-01-01T16:00:00.000Z",
  "dateModified": "2026-01-02T02:36:16.931Z",
  "author": [
    {
      "@type": "Person",
      "name": "GuoDong",
      "url": "https://gaoguodong03.github.io/gdBlog"
    }
  ]
}</script><link rel="shortcut icon" href="/gdBlog/img/logo.jpg"><link rel="canonical" href="https://gaoguodong03.github.io/gdBlog/2026/01/02/RLHF%EF%BC%88Reinforcement-Learning-from-Human-Feedback%EF%BC%8C%E5%9F%BA%E4%BA%8E%E4%BA%BA%E7%B1%BB%E5%8F%8D%E9%A6%88%E7%9A%84%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%89/index.html"><link rel="preconnect"/><link rel="stylesheet" href="/gdBlog/css/index.css"><link rel="stylesheet" href="/gdBlog/pluginsSrc/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/gdBlog/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: '/gdBlog/pluginsSrc/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'RLHF（Reinforcement Learning from Human Feedback，基于人类反馈的强化学习）',
  isHighlightShrink: true,
  isToc: true,
  pageType: 'post'
}</script><style>#article-container.post-content h1:before, h2:before, h3:before, h4:before, h5:before, h6:before { -webkit-animation: avatar_turn_around 1s linear infinite; -moz-animation: avatar_turn_around 1s linear infinite; -o-animation: avatar_turn_around 1s linear infinite; -ms-animation: avatar_turn_around 1s linear infinite; animation: avatar_turn_around 1s linear infinite; }</style><!-- hexo injector head_end start --><link rel="stylesheet" href="/gdBlog/./css/categorybar.css"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiperstyle.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-wowjs/lib/animate.min.css" media="print" onload="this.media='screen'"><!-- hexo injector head_end end --><meta name="generator" content="Hexo 8.0.0"></head><body><div id="web_bg" style="background-image: url(/gdBlog/img/bg1.jpg);"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/gdBlog/img/touxiang.jpg" onerror="this.onerror=null;this.src='/gdBlog/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/gdBlog/archives/"><div class="headline">文章</div><div class="length-num">29</div></a><a href="/gdBlog/tags/"><div class="headline">标签</div><div class="length-num">20</div></a><a href="/gdBlog/categories/"><div class="headline">分类</div><div class="length-num">7</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/gdBlog/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/gdBlog/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/gdBlog/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/gdBlog/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/gdBlog/link/"><i class="fa-fw fas fa-link"></i><span> 书签</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(/gdBlog/./img/);"><nav id="nav"><!-- 左侧博客信息区域--><span id="blog-info"><a class="nav-site-title" href="/gdBlog/"><img class="site-icon" src="/gdBlog/img/logo.jpg" alt="Logo"><span class="site-name">果冻小配方</span></a><a class="nav-page-title" href="/gdBlog/"><span class="site-name">RLHF（Reinforcement Learning from Human Feedback，基于人类反馈的强化学习）</span></a></span><!-- 新增的导航菜单容器（居中布局关键）--><div id="nav-menus-container"><!-- 菜单主体部分--><div id="menus"><!-- 搜索按钮--><!-- 菜单项--><div class="menus_items"><div class="menus_item"><a class="site-page" href="/gdBlog/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/gdBlog/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/gdBlog/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/gdBlog/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/gdBlog/link/"><i class="fa-fw fas fa-link"></i><span> 书签</span></a></div></div></div><!-- 移动端汉堡菜单按钮（保持原位置）--><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">RLHF（Reinforcement Learning from Human Feedback，基于人类反馈的强化学习）</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2026-01-01T16:00:00.000Z" title="发表于 2026-01-02 00:00:00">2026-01-02</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2026-01-02T02:36:16.931Z" title="更新于 2026-01-02 10:36:16">2026-01-02</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/gdBlog/categories/%E6%9E%9C%E5%86%BB%E7%9A%84%E7%A7%91%E6%99%AE%E4%B8%93%E5%8C%BA/">果冻的科普专区</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">总字数:</span><span class="word-count">698</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>2分钟</span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><p><strong>RLHF（Reinforcement Learning from Human Feedback，基于人类反馈的强化学习）</strong> 是一种用于训练机器学习模型（特别是大型语言模型）的技术方法。它的核心思想是让模型通过与人类的互动反馈来学习和优化自身行为。</p>
<h3 id="主要流程"><a href="#主要流程" class="headerlink" title="主要流程"></a><strong>主要流程</strong></h3><p>RLHF 通常包含以下三个关键步骤：</p>
<ol>
<li><p><strong>监督微调（Supervised Fine-Tuning, SFT）</strong></p>
<ul>
<li>首先，在高质量的人类标注数据上对预训练的基础模型进行微调，使其初步学会按照人类期望的方式回答问题。</li>
</ul>
</li>
<li><p><strong>奖励模型训练（Reward Modeling, RM）</strong></p>
<ul>
<li>人类评估员对模型的多个输出进行排序（例如，比较两个回答哪个更好）。</li>
<li>根据这些偏好数据，训练一个独立的“奖励模型”（Reward Model），使其能够自动判断模型输出质量的高低（给出分数）。</li>
</ul>
</li>
<li><p><strong>强化学习微调（Reinforcement Learning Fine-Tuning）</strong></p>
<ul>
<li>将第一步的 SFT 模型作为初始策略模型。</li>
<li>使用第二步训练好的奖励模型作为“评分标准&#x2F;奖励函数”。</li>
<li>通过强化学习算法（如 PPO，近端策略优化）来优化策略模型，目标是<strong>最大化奖励模型给出的奖励分数</strong>。</li>
<li>这个过程相当于让模型在不断“试错”中，学会产生更高奖励分数（即更符合人类偏好）的回答。</li>
</ul>
</li>
</ol>
<h3 id="为什么重要？"><a href="#为什么重要？" class="headerlink" title="为什么重要？"></a><strong>为什么重要？</strong></h3><ul>
<li><strong>对齐问题</strong>：RLHF 是解决 AI <strong>对齐问题（Alignment Problem）</strong> 的核心技术之一。它让模型的输出目标与人类复杂的价值观、意图和偏好保持一致，而不仅仅是根据训练数据预测下一个词。</li>
<li><strong>超越模仿</strong>：相比仅模仿数据的监督学习，RLHF 能引导模型优化一些难以直接标注的、更抽象的目标，如“有帮助性”、“安全性”、“诚实性”和“无害性”。</li>
<li><strong>主流应用</strong>：这已经成为训练 ChatGPT、Claude 等先进对话 AI 的核心方法，使其能生成更符合人类期待且安全可靠的回答。</li>
</ul>
<h3 id="面临的挑战"><a href="#面临的挑战" class="headerlink" title="面临的挑战"></a><strong>面临的挑战</strong></h3><ul>
<li><strong>奖励模型的好坏决定上限</strong>：如果奖励模型无法准确反映人类复杂偏好，强化学习可能会“钻空子”，产生高分但不符合期望的行为（奖励黑客，Reward Hacking）。</li>
<li><strong>成本高昂</strong>：依赖大量高质量的人类反馈数据，标注过程费时费力。</li>
<li><strong>可能限制创造力</strong>：过度优化可能使模型回答过于保守或模式化。</li>
</ul>
<p><strong>简而言之，RLHF 是一种让 AI 模型“听人话、学人好”的高级训练方法，通过人类打分来教它什么是更好的表现，使其行为与我们希望的价值观和目标任务对齐。</strong></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://gaoguodong03.github.io/gdBlog">GuoDong</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://gaoguodong03.github.io/gdBlog/2026/01/02/RLHF%EF%BC%88Reinforcement-Learning-from-Human-Feedback%EF%BC%8C%E5%9F%BA%E4%BA%8E%E4%BA%BA%E7%B1%BB%E5%8F%8D%E9%A6%88%E7%9A%84%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%89/">https://gaoguodong03.github.io/gdBlog/2026/01/02/RLHF%EF%BC%88Reinforcement-Learning-from-Human-Feedback%EF%BC%8C%E5%9F%BA%E4%BA%8E%E4%BA%BA%E7%B1%BB%E5%8F%8D%E9%A6%88%E7%9A%84%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%89/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://gaoguodong03.github.io/gdBlog" target="_blank">果冻小配方</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/gdBlog/tags/%E8%B0%83%E7%A0%94/">调研</a><a class="post-meta__tags" href="/gdBlog/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/">强化学习</a><a class="post-meta__tags" href="/gdBlog/tags/%E7%A7%91%E6%99%AE/">科普</a></div><div class="post-share"><div class="social-share" data-image="/gdBlog/img/touxiang.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="/gdBlog/pluginsSrc/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="/gdBlog/pluginsSrc/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/gdBlog/2026/01/02/%E6%8F%90%E7%A4%BA%E5%B7%A5%E7%A8%8B%E8%B0%83%E7%A0%94/" title="提示工程调研"><img class="cover" src="/gdBlog/./img/logoZiyouzhiyi.jpg" onerror="onerror=null;src='/gdBlog/img/404.jpg'" alt="cover of previous post"><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">提示工程调研</div></div><div class="info-2"><div class="info-item-1">写在前面  文档链接 open ai 持续强化 ChatGPT 图谱以抵御即时注入攻击 [[..&#x2F;img&#x2F;logoZiyouzhiyi.jpg]][[基于提示词工程的通用大模型]] 参考文献提示工程指南 | Prompt Engineering Guide Google 《Prompt Engineering提示词手册》Prompt Engineering_v7.pdf 综述：大语言模型 THE CHINESE BOOK FOR LARGE LANGUAGE MODELS 内容速览一些编写提示词的建议 一些提示技术和思想 快速使用–好用的提示词框架—-&gt;****提示工程调研 一些值得思考的问题 提示工程简介定义：针对特定任务设计合适的任务提示,这一过程被称为提示工程。 提示工程(Prompt Engineering)是一门相对较新的学科，伴随大语言模型快速发展而兴起。提示工程不仅仅是关于设计提示词，它包含了与大语言模型交互和研发的各种技能和技术。 出现原因 同一个模型，通过不同的提示方式，效果差异巨大  低成本、快速迭代、无需训练、对所有人开放   提示...</div></div></div></a><a class="pagination-related" href="/gdBlog/2026/01/02/GRPO%EF%BC%88Group-Relative-Policy-Optimization%EF%BC%8C%E5%88%86%E7%BB%84%E7%9B%B8%E5%AF%B9%E7%AD%96%E7%95%A5%E4%BC%98%E5%8C%96%EF%BC%89/" title="GRPO（Group Relative Policy Optimization，分组相对策略优化）"><img class="cover" src="/gdBlog/./img/logoLLM.png" onerror="onerror=null;src='/gdBlog/img/404.jpg'" alt="cover of next post"><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">GRPO（Group Relative Policy Optimization，分组相对策略优化）</div></div><div class="info-2"><div class="info-item-1">GRPO（Group Relative Policy Optimization，分组相对策略优化） 是 DeepSeek 在 V3 模型训练中引入的一种无奖励模型的强化学习对齐技术。它是一种 RLHF（人类反馈强化学习）的变体或改进方法，旨在更高效、更稳定地训练模型与人类偏好对齐。[[RLHF（Reinforcement Learning from Human Feedback，基于人类反馈的强化学习）|RLHF（Reinforcement Learning from Human Feedback，基于人类反馈的强化学习）]] 核心思路GRPO 的核心创新在于摆脱了对独立奖励模型的依赖。传统的 RLHF 需要先训练一个复杂的奖励模型来评估生成内容，而 GRPO 直接利用人类偏好数据进行端到端的策略优化。 工作原理（简化版） 分组对比：  对于同一个问题（提示），模型会生成一组（例如 4 个）不同的回答。 这些回答根据质量被模型或参考标准排序。   相对偏好建模：  GRPO 的核心是直接比较同一组内回答的相对好坏，而不是像传统 RLHF 那样依赖一个绝对分数预测的奖励模型。 它通...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/gdBlog/2026/01/02/GRPO%EF%BC%88Group-Relative-Policy-Optimization%EF%BC%8C%E5%88%86%E7%BB%84%E7%9B%B8%E5%AF%B9%E7%AD%96%E7%95%A5%E4%BC%98%E5%8C%96%EF%BC%89/" title="GRPO（Group Relative Policy Optimization，分组相对策略优化）"><img class="cover" src="/gdBlog/./img/logoLLM.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2026-01-02</div><div class="info-item-2">GRPO（Group Relative Policy Optimization，分组相对策略优化）</div></div><div class="info-2"><div class="info-item-1">GRPO（Group Relative Policy Optimization，分组相对策略优化） 是 DeepSeek 在 V3 模型训练中引入的一种无奖励模型的强化学习对齐技术。它是一种 RLHF（人类反馈强化学习）的变体或改进方法，旨在更高效、更稳定地训练模型与人类偏好对齐。[[RLHF（Reinforcement Learning from Human Feedback，基于人类反馈的强化学习）|RLHF（Reinforcement Learning from Human Feedback，基于人类反馈的强化学习）]] 核心思路GRPO 的核心创新在于摆脱了对独立奖励模型的依赖。传统的 RLHF 需要先训练一个复杂的奖励模型来评估生成内容，而 GRPO 直接利用人类偏好数据进行端到端的策略优化。 工作原理（简化版） 分组对比：  对于同一个问题（提示），模型会生成一组（例如 4 个）不同的回答。 这些回答根据质量被模型或参考标准排序。   相对偏好建模：  GRPO 的核心是直接比较同一组内回答的相对好坏，而不是像传统 RLHF 那样依赖一个绝对分数预测的奖励模型。 它通...</div></div></div></a><a class="pagination-related" href="/gdBlog/2026/01/02/2025%E5%B9%B4%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%8E%B0%E7%8A%B6%EF%BC%9A%E8%BF%9B%E5%B1%95%E3%80%81%E9%97%AE%E9%A2%98%E4%B8%8E%E9%A2%84%E6%B5%8B/" title="2025年大型语言模型现状：进展、问题与预测"><img class="cover" src="/gdBlog/./img/logoLLM.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2026-01-02</div><div class="info-item-2">2025年大型语言模型现状：进展、问题与预测</div></div><div class="info-2"><div class="info-item-1">原文链接 随着2025年即将结束，我想回顾今年大型语言模型中一些最重要的发展，反思那些仍然存在的局限性和未解决的问题，并分享一些对未来可能出现的展望。 1. 推理之年、RLVR 和 GRPO缩放依然有效，但实际上并没有改变大型语言模型的行为或实际体验（唯一的例外是 OpenAI 新发布的 o1，它增加了推理痕迹）。因此，当 DeepSeek 在 2025 年 1 月发布他们的 R1 论文 ，证明通过强化学习可以发展类推理行为时，这成为了一件大事。（在大型语言模型（LLM）中，推理意味着模型解释其答案，而这种解释本身通常会提高答案的准确性。） 1.1 The DeepSeek MomentDeepSeek R1 因多种原因备受关注： 首先，DeepSeek R1 作为一个开权重模型发布，表现非常好，可与当时最好的专有模型（ChatGPT、Gemini 等）相媲美。 其次，DeepSeek R1 论文促使许多人，尤其是投资者和记者，重新审视了 2024 年 12 月的 DeepSeek V3 早期论文 。这导致了一个修正的结论：虽然训练最先进的模型仍然昂贵，但可能比之前假设便宜一个数...</div></div></div></a><a class="pagination-related" href="/gdBlog/2025/10/01/DeepSearch%E7%9A%84%E8%AE%BA%E6%96%87%E8%B0%83%E7%A0%94/" title="DeepSearch的论文调研"><img class="cover" src="/gdBlog/./img/logoZiyouzhiyi.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-10-01</div><div class="info-item-2">DeepSearch的论文调研</div></div><div class="info-2"><div class="info-item-1">论文调研综述论文A Comprehensive Survey of Deep Research: Systems, Methodologies, and Applications该综述由浙江大学徐仁军和彭静雯撰写，系统研究了Deep Research 系统这一快速发展的领域 —— 这类系统通过整合大型语言模型（LLMs）、先进信息检索技术和自主推理能力，实现复杂研究工作流的自动化；综述分析了 2023 年以来出现的80 多个商业和非商业系统（如 OpenAI&#x2F;DeepResearch、Gemini&#x2F;DeepResearch 等），提出了基于 “基础模型与推理引擎、工具利用与环境交互、任务规划与执行控制、知识合成与输出生成” 的四层技术分类体系，探讨了系统在学术、科学、商业、教育等领域的架构模式与应用适配性，指出当前系统在信息准确性、隐私、知识产权等方面的技术与伦理挑战，并明确了先进推理架构、多模态融合、领域专业化等未来研究方向，同时提供了相关资源库（https://github.com/scienceaix/deepresearch）支持进一步研究 ![[Ob...</div></div></div></a><a class="pagination-related" href="/gdBlog/2025/10/11/%E4%B8%8A%E4%B8%8B%E6%96%87%E5%B7%A5%E7%A8%8B-Context-Engineering/" title="上下文工程(Context Engineering)"><img class="cover" src="/gdBlog/./img/logoZiyouzhiyi.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-10-11</div><div class="info-item-2">上下文工程(Context Engineering)</div></div><div class="info-2"><div class="info-item-1">前言随着大模型能力的快速发展，人们对大语言模型的使用方式越来越多的从简单的ChatBot，变成了各种Agent，而一个新的术语——上下文工程（Context Engineering），也逐步取代了[[提示工程]](Prompt Engineering）。 最近[Anthropic]发表了一篇文章，分享了他们在上下文工程上的探索，本文基于原文做了适当解读。 上下文工程和提示工程的区别—记忆简单来说，提示工程重点关注的是为了获得最佳结果而编写和组织的LLM指令的方法，各种在公众号、B站、小红书上所介绍的如何在某一款软件中输入特定的指令从而能够获得某种效果的方法，都属于提示工程，它通常涉及需要完成的任务、任务的背景信息、如何更好的完成任务、哪些是禁止的、受众是谁等，例如如何使用AI写文章、如何去除AI味、如何使用豆包生成特定的图片等。 而上下文工程指的是，在LLM推理过程中，如何管理和维护要输入给LLM的最佳信息的策略集，之所以叫信息，而没有叫Prompt或者提示语，是因为在上下文工程中，除了涉及给LLM安排任务，还涉及到工具、当前任务所需要的参考资料、之前交互过程中的记忆等，参考资料...</div></div></div></a><a class="pagination-related" href="/gdBlog/2025/10/11/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%8D%8F%E5%90%8C%E6%9E%B6%E6%9E%84%E8%B0%83%E7%A0%94/" title="多智能体协同架构调研"><img class="cover" src="/gdBlog/./img/logoZiyouzhiyi.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-10-11</div><div class="info-item-2">多智能体协同架构调研</div></div><div class="info-2"><div class="info-item-1">上下文工程 任务管理者 依次往总线扔任务 会议时间文档链接：https://free4inno.feishu.cn/wiki/CxV0wuVATi8Z0hk6wF8cWgzCnTb报告时间：2025&#x2F;10&#x2F;10报告人：孟千斌 简要记录调研了多种多智能体协同架构，包括 OpenManus、微软开源的 AutoGen、字节开源的 LongManus（基于 LongGraph）等，并对它们进行了横向对比  OpenManus：因闭源，分析基于官网 Blog 及演示视频，聚焦其多智能体协同模式，单智能体模式未涉及。 AutoGen：提供 5 种多智能体协同解法： 双智能体聊天（带函数调用）：由用户代理和助手构成，通过交替对话、任务拆分函数调用逐步完成任务，支持人工在关键节点介入。 群聊（Group chat）：预设多角色智能体与群聊管理器，管理器根据上下文和角色能力自动路由任务，公共历史记录信息，支持发布 &#x2F; 订阅模式优化。 AutoBuild：能从自然语言需求出发，自动生成带角色定位的智能体并组成群聊，经协作完成任务后反思总结并返回结果。 Mixture ...</div></div></div></a><a class="pagination-related" href="/gdBlog/2026/01/02/%E6%8F%90%E7%A4%BA%E5%B7%A5%E7%A8%8B%E8%B0%83%E7%A0%94/" title="提示工程调研"><img class="cover" src="/gdBlog/./img/logoZiyouzhiyi.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2026-01-02</div><div class="info-item-2">提示工程调研</div></div><div class="info-2"><div class="info-item-1">写在前面  文档链接 open ai 持续强化 ChatGPT 图谱以抵御即时注入攻击 [[..&#x2F;img&#x2F;logoZiyouzhiyi.jpg]][[基于提示词工程的通用大模型]] 参考文献提示工程指南 | Prompt Engineering Guide Google 《Prompt Engineering提示词手册》Prompt Engineering_v7.pdf 综述：大语言模型 THE CHINESE BOOK FOR LARGE LANGUAGE MODELS 内容速览一些编写提示词的建议 一些提示技术和思想 快速使用–好用的提示词框架—-&gt;****提示工程调研 一些值得思考的问题 提示工程简介定义：针对特定任务设计合适的任务提示,这一过程被称为提示工程。 提示工程(Prompt Engineering)是一门相对较新的学科，伴随大语言模型快速发展而兴起。提示工程不仅仅是关于设计提示词，它包含了与大语言模型交互和研发的各种技能和技术。 出现原因 同一个模型，通过不同的提示方式，效果差异巨大  低成本、快速迭代、无需训练、对所有人开放   提示...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/gdBlog/img/touxiang.jpg" onerror="this.onerror=null;this.src='/gdBlog/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">GuoDong</div><div class="author-info-description">碎碎念念 岁岁年年</div><div class="site-data"><a href="/gdBlog/archives/"><div class="headline">文章</div><div class="length-num">29</div></a><a href="/gdBlog/tags/"><div class="headline">标签</div><div class="length-num">20</div></a><a href="/gdBlog/categories/"><div class="headline">分类</div><div class="length-num">7</div></a></div><div class="card-info-social-icons"><a class="social-icon" href="https://www.douyin.com/" target="_blank" title="douyin"><i class="fab fa-tiktok" style="color: #24292e;"></i></a><a class="social-icon" href="https://www.bilibili.com/" target="_blank" title="bilibili"><i class="fab fa-bilibili" style="color: #24292e;"></i></a><a class="social-icon" href="https://github.com/gaoguodong03" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="https://leetcode.cn/studyplan/top-100-liked/" target="_blank" title="LeetCode"><i class="fab fa-comments" style="color: #24292e;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">有重要的事情 得去趟南京<br>苦苦刷LeetCode<br>常驻实验室选手<br>健身房小小白</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BB%E8%A6%81%E6%B5%81%E7%A8%8B"><span class="toc-number">1.</span> <span class="toc-text">主要流程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E9%87%8D%E8%A6%81%EF%BC%9F"><span class="toc-number">2.</span> <span class="toc-text">为什么重要？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%9D%A2%E4%B8%B4%E7%9A%84%E6%8C%91%E6%88%98"><span class="toc-number">3.</span> <span class="toc-text">面临的挑战</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/gdBlog/2026/01/02/2025%E5%B9%B4%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%8E%B0%E7%8A%B6%EF%BC%9A%E8%BF%9B%E5%B1%95%E3%80%81%E9%97%AE%E9%A2%98%E4%B8%8E%E9%A2%84%E6%B5%8B/" title="2025年大型语言模型现状：进展、问题与预测">2025年大型语言模型现状：进展、问题与预测</a><time datetime="2026-01-01T16:00:00.000Z" title="发表于 2026-01-02 00:00:00">2026-01-02</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/gdBlog/2026/01/02/GRPO%EF%BC%88Group-Relative-Policy-Optimization%EF%BC%8C%E5%88%86%E7%BB%84%E7%9B%B8%E5%AF%B9%E7%AD%96%E7%95%A5%E4%BC%98%E5%8C%96%EF%BC%89/" title="GRPO（Group Relative Policy Optimization，分组相对策略优化）">GRPO（Group Relative Policy Optimization，分组相对策略优化）</a><time datetime="2026-01-01T16:00:00.000Z" title="发表于 2026-01-02 00:00:00">2026-01-02</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/gdBlog/2026/01/02/RLHF%EF%BC%88Reinforcement-Learning-from-Human-Feedback%EF%BC%8C%E5%9F%BA%E4%BA%8E%E4%BA%BA%E7%B1%BB%E5%8F%8D%E9%A6%88%E7%9A%84%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%89/" title="RLHF（Reinforcement Learning from Human Feedback，基于人类反馈的强化学习）">RLHF（Reinforcement Learning from Human Feedback，基于人类反馈的强化学习）</a><time datetime="2026-01-01T16:00:00.000Z" title="发表于 2026-01-02 00:00:00">2026-01-02</time></div></div></div></div></div></div></main><footer id="footer" style="background: transparent;"><div class="footer-other"><div class="footer-copyright"></div><div class="footer_custom_text"><style>
  .footer {
    text-align: center;
    position: relative;
  }
  .social-links {
    display: flex;
    justify-content: center;
    gap: 1.5rem;
    flex-wrap: wrap;
  }
  .social-links i {
    color: #000000;
  }
  .social-link {
    color: #000000;
    font-size: 1.2rem;
    transition: all 0.3s ease;
    display: inline-flex;
    align-items: center;
    justify-content: center;
    width: 2.5rem;
    height: 2.5rem;
    border-radius: 50%;
    background: rgba(0, 0, 0, 0.1);
  }
  .social-link:hover {
    color: #333333;
    background: rgba(0, 0, 0, 0.2);
    transform: translateY(-3px) scale(1.2);
    box-shadow: 0 5px 15px rgba(0, 0, 0, 0.3);
    text-decoration: none !important; 
  }
  .footer p {
    margin: 0.5rem 0;
    line-height: 1;
  }
  .copyright {
    font-size: 1.1rem; 
    color: #000000;
    font-weight: 400;
  }
  .tagline {
    font-size: 0.8rem;
    color: #333333;
    font-style: italic;
    font-weight: 500;
  }
  .visitor-count {
    font-size: 0.75rem;
    color: rgba(0, 0, 0, 0.7);
    font-weight: 300;
  }
  #visitorCount {
    font-weight: bold;
  }
</style>
<div class="footer">
  <p class="copyright">© 2025 果冻小配方 - 所有权利保留</p>
  <p class="tagline">> 碎碎念念 岁岁年年 <</p>
  <p class="visitor-count">访问量: <span id="visitorCount">1024</span> | 你是第 <span id="dailyVisitor">1</span> 位今日访客</p>
</div>
<script>
  // 确保DOM加载完成后执行
  document.addEventListener('DOMContentLoaded', function() {
    // 模拟访问量增长
    function updateVisitorCount() {
      const countElement = document.getElementById('visitorCount');
      let count = parseInt(countElement.textContent) || 1024;
      // 从localStorage获取或初始化计数
      const storedCount = localStorage.getItem('totalVisitors');
      if (storedCount) {
        count = parseInt(storedCount);
        countElement.textContent = count;
      }
      // 每日访客计数
      const today = new Date().toDateString();
      const dailyData = JSON.parse(localStorage.getItem('dailyVisitors') || '{"date":"", "count":0}');
      if (dailyData.date !== today) {
        dailyData.date = today;
        dailyData.count = 0;
      }
      dailyData.count += 1;
      document.getElementById('dailyVisitor').textContent = dailyData.count;
      localStorage.setItem('dailyVisitors', JSON.stringify(dailyData));
      // 每30秒随机增加访问量
      setInterval(() => {
        count += Math.floor(Math.random() * 3);
        countElement.textContent = count;
        localStorage.setItem('totalVisitors', count.toString());
      }, 30000);
    }
    updateVisitorCount();
    // 添加点击动画效果
    const socialLinks = document.querySelectorAll('.social-link');
    socialLinks.forEach(link => {
      link.addEventListener('click', function() {
        this.style.transform = 'scale(0.9)';
        setTimeout(() => {
          this.style.transform = '';
        }, 300);
      });
    });
  });
</script>
</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/gdBlog/js/utils.js"></script><script src="/gdBlog/js/main.js"></script><div class="js-pjax"></div><script src="/config/js/happy-title.js" async></script><script src="/config/js/foot.js" async></script></div><!-- hexo injector body_end start --><script data-pjax>
  function butterfly_categories_card_injector_config(){
    // 检查容器是否存在
    var parent_div_git = document.getElementById('recent-posts');
    // 如果容器不存在，则动态创建
    if (!parent_div_git) {
      console.warn('butterfly_categories_card: 挂载容器不存在，正在动态创建...');
      // 创建新容器（默认插入到页面主体顶部）
      parent_div_git = document.createElement('div');
      parent_div_git.id = 'recent-posts'; // 赋予配置的ID
      document.querySelector('#page').prepend(parent_div_git); // 插入到 #content-inner 内
    }
    var item_html = '<style>li.categoryBar-list-item{width:32.3%;}.categoryBar-list{max-height: 950px;overflow:auto;}.categoryBar-list::-webkit-scrollbar{width:0!important}@media screen and (max-width: 650px){.categoryBar-list{max-height: 800px;}}</style><div class="recent-post-item" style="height:auto;width:100%;padding:0px;"><div id="categoryBar"><ul class="categoryBar-list"><li class="categoryBar-list-item" style="background:url(/img/logo.jpg);"> <a class="categoryBar-list-link" href="/categories/果冻的日记本/">果冻的日记本</a><span class="categoryBar-list-count">4</span><span class="categoryBar-list-descr">果冻的航海日记</span></li><li class="categoryBar-list-item" style="background:url(/img/logo.jpg);"> <a class="categoryBar-list-link" href="/categories/果冻的理论学习/">果冻的理论学习</a><span class="categoryBar-list-count">10</span><span class="categoryBar-list-descr">Hexo</span></li><li class="categoryBar-list-item" style="background:url(/img/logo.jpg);"> <a class="categoryBar-list-link" href="/categories/果冻的奇妙小工具/">果冻的奇妙小工具</a><span class="categoryBar-list-count">5</span><span class="categoryBar-list-descr">果冻的奇妙小工具</span></li><li class="categoryBar-list-item" style="background:url(/img/logo.jpg);"> <a class="categoryBar-list-link" href="/categories/果冻的科普专区/">果冻的科普专区</a><span class="categoryBar-list-count">3</span><span class="categoryBar-list-descr">果冻的日记本</span></li><li class="categoryBar-list-item" style="background:url(undefined);"> <a class="categoryBar-list-link" href="/categories/果冻的航海日志/">果冻的航海日志</a><span class="categoryBar-list-count">5</span><span class="categoryBar-list-descr"></span></li><li class="categoryBar-list-item" style="background:url(undefined);"> <a class="categoryBar-list-link" href="/categories/Hexo/">Hexo</a><span class="categoryBar-list-count">1</span><span class="categoryBar-list-descr"></span></li><li class="categoryBar-list-item" style="background:url(undefined);"> <a class="categoryBar-list-link" href="/categories/果冻的LeetCode刷题/">果冻的LeetCode刷题</a><span class="categoryBar-list-count">5</span><span class="categoryBar-list-descr"></span></li></ul></div></div>';
    console.log('已挂载 butterfly_categories_card');
    parent_div_git.insertAdjacentHTML("afterbegin",item_html)
  }
  // 路径匹配逻辑（使用 startsWith）
  if (location.pathname.startsWith('/categories/') || '/categories/' === 'all') {
    butterfly_categories_card_injector_config();
  }
  </script><script data-pjax>
  function butterfly_swiper_injector_config(){
    var parent_div_git = document.getElementById('recent-posts');
    var item_html = '<div class="recent-post-item" style="height: auto;width: 100%"><div class="blog-slider swiper-container-fade swiper-container-horizontal" id="swiper_container"><div class="blog-slider__wrp swiper-wrapper" style="transition-duration: 0ms;"></div><div class="blog-slider__pagination swiper-pagination-clickable swiper-pagination-bullets"></div></div></div>';
    console.log('已挂载butterfly_swiper')
    parent_div_git.insertAdjacentHTML("afterbegin",item_html)
    }
  var elist = 'undefined'.split(',');
  var cpage = location.pathname;
  var epage = '/';
  var flag = 0;

  for (var i=0;i<elist.length;i++){
    if (cpage.includes(elist[i])){
      flag++;
    }
  }

  if ((epage ==='all')&&(flag == 0)){
    butterfly_swiper_injector_config();
  }
  else if (epage === cpage){
    butterfly_swiper_injector_config();
  }
  </script><script defer src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.js"></script><script defer data-pjax src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper_init.js"></script><div class="js-pjax"><script async="async">var arr = document.getElementsByClassName('recent-post-item');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__zoomIn');
    arr[i].setAttribute('data-wow-duration', '300ms');
    arr[i].setAttribute('data-wow-delay', '0ms');
    arr[i].setAttribute('data-wow-offset', '0');
    arr[i].setAttribute('data-wow-iteration', '1');
  }</script><script async="async">var arr = document.getElementsByClassName('card-widget');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__zoomIn');
    arr[i].setAttribute('data-wow-duration', '');
    arr[i].setAttribute('data-wow-delay', '');
    arr[i].setAttribute('data-wow-offset', '');
    arr[i].setAttribute('data-wow-iteration', '');
  }</script></div><script defer src="https://npm.elemecdn.com/hexo-butterfly-wowjs/lib/wow.min.js"></script><script defer src="https://npm.elemecdn.com/hexo-butterfly-wowjs/lib/wow_init.js"></script><!-- hexo injector body_end end --></body></html>