<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>2025年大型语言模型现状：进展、问题与预测 | 果冻小配方</title><meta name="author" content="GuoDong"><meta name="copyright" content="GuoDong"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="原文链接 随着2025年即将结束，我想回顾今年大型语言模型中一些最重要的发展，反思那些仍然存在的局限性和未解决的问题，并分享一些对未来可能出现的展望。 1. 推理之年、RLVR 和 GRPO缩放依然有效，但实际上并没有改变大型语言模型的行为或实际体验（唯一的例外是 OpenAI 新发布的 o1，它增加了推理痕迹）。因此，当 DeepSeek 在 2025 年 1 月发布他们的 R1 论文 ，证明通">
<meta property="og:type" content="article">
<meta property="og:title" content="2025年大型语言模型现状：进展、问题与预测">
<meta property="og:url" content="https://gaoguodong03.github.io/gdBlog/2026/01/02/2025%E5%B9%B4%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%8E%B0%E7%8A%B6%EF%BC%9A%E8%BF%9B%E5%B1%95%E3%80%81%E9%97%AE%E9%A2%98%E4%B8%8E%E9%A2%84%E6%B5%8B/index.html">
<meta property="og:site_name" content="果冻小配方">
<meta property="og:description" content="原文链接 随着2025年即将结束，我想回顾今年大型语言模型中一些最重要的发展，反思那些仍然存在的局限性和未解决的问题，并分享一些对未来可能出现的展望。 1. 推理之年、RLVR 和 GRPO缩放依然有效，但实际上并没有改变大型语言模型的行为或实际体验（唯一的例外是 OpenAI 新发布的 o1，它增加了推理痕迹）。因此，当 DeepSeek 在 2025 年 1 月发布他们的 R1 论文 ，证明通">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://gaoguodong03.github.io/gdBlog/img/logoLLM.png">
<meta property="article:published_time" content="2026-01-01T16:00:00.000Z">
<meta property="article:modified_time" content="2026-01-02T03:12:11.511Z">
<meta property="article:author" content="GuoDong">
<meta property="article:tag" content="LLM">
<meta property="article:tag" content="强化学习">
<meta property="article:tag" content="总结">
<meta property="article:tag" content="调研">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://gaoguodong03.github.io/gdBlog/img/logoLLM.png"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "2025年大型语言模型现状：进展、问题与预测",
  "url": "https://gaoguodong03.github.io/gdBlog/2026/01/02/2025%E5%B9%B4%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%8E%B0%E7%8A%B6%EF%BC%9A%E8%BF%9B%E5%B1%95%E3%80%81%E9%97%AE%E9%A2%98%E4%B8%8E%E9%A2%84%E6%B5%8B/",
  "image": "https://gaoguodong03.github.io/gdBlog/img/logoLLM.png",
  "datePublished": "2026-01-01T16:00:00.000Z",
  "dateModified": "2026-01-02T03:12:11.511Z",
  "author": [
    {
      "@type": "Person",
      "name": "GuoDong",
      "url": "https://gaoguodong03.github.io/gdBlog"
    }
  ]
}</script><link rel="shortcut icon" href="/gdBlog/img/logo.jpg"><link rel="canonical" href="https://gaoguodong03.github.io/gdBlog/2026/01/02/2025%E5%B9%B4%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%8E%B0%E7%8A%B6%EF%BC%9A%E8%BF%9B%E5%B1%95%E3%80%81%E9%97%AE%E9%A2%98%E4%B8%8E%E9%A2%84%E6%B5%8B/index.html"><link rel="preconnect"/><link rel="stylesheet" href="/gdBlog/css/index.css"><link rel="stylesheet" href="/gdBlog/pluginsSrc/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/gdBlog/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: '/gdBlog/pluginsSrc/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '2025年大型语言模型现状：进展、问题与预测',
  isHighlightShrink: true,
  isToc: true,
  pageType: 'post'
}</script><style>#article-container.post-content h1:before, h2:before, h3:before, h4:before, h5:before, h6:before { -webkit-animation: avatar_turn_around 1s linear infinite; -moz-animation: avatar_turn_around 1s linear infinite; -o-animation: avatar_turn_around 1s linear infinite; -ms-animation: avatar_turn_around 1s linear infinite; animation: avatar_turn_around 1s linear infinite; }</style><!-- hexo injector head_end start --><link rel="stylesheet" href="/gdBlog/./css/categorybar.css"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiperstyle.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-wowjs/lib/animate.min.css" media="print" onload="this.media='screen'"><!-- hexo injector head_end end --><meta name="generator" content="Hexo 8.0.0"></head><body><div id="web_bg" style="background-image: url(/gdBlog/img/bg1.jpg);"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/gdBlog/img/touxiang.jpg" onerror="this.onerror=null;this.src='/gdBlog/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/gdBlog/archives/"><div class="headline">文章</div><div class="length-num">29</div></a><a href="/gdBlog/tags/"><div class="headline">标签</div><div class="length-num">20</div></a><a href="/gdBlog/categories/"><div class="headline">分类</div><div class="length-num">7</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/gdBlog/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/gdBlog/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/gdBlog/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/gdBlog/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/gdBlog/link/"><i class="fa-fw fas fa-link"></i><span> 书签</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(/gdBlog/./img/logoLLM.png);"><nav id="nav"><!-- 左侧博客信息区域--><span id="blog-info"><a class="nav-site-title" href="/gdBlog/"><img class="site-icon" src="/gdBlog/img/logo.jpg" alt="Logo"><span class="site-name">果冻小配方</span></a><a class="nav-page-title" href="/gdBlog/"><span class="site-name">2025年大型语言模型现状：进展、问题与预测</span></a></span><!-- 新增的导航菜单容器（居中布局关键）--><div id="nav-menus-container"><!-- 菜单主体部分--><div id="menus"><!-- 搜索按钮--><!-- 菜单项--><div class="menus_items"><div class="menus_item"><a class="site-page" href="/gdBlog/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/gdBlog/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/gdBlog/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/gdBlog/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/gdBlog/link/"><i class="fa-fw fas fa-link"></i><span> 书签</span></a></div></div></div><!-- 移动端汉堡菜单按钮（保持原位置）--><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">2025年大型语言模型现状：进展、问题与预测</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2026-01-01T16:00:00.000Z" title="发表于 2026-01-02 00:00:00">2026-01-02</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2026-01-02T03:12:11.511Z" title="更新于 2026-01-02 11:12:11">2026-01-02</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/gdBlog/categories/%E6%9E%9C%E5%86%BB%E7%9A%84%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/">果冻的理论学习</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">总字数:</span><span class="word-count">4.7k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>14分钟</span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><p><a target="_blank" rel="noopener" href="https://magazine.sebastianraschka.com/p/state-of-llms-2025">原文链接</a></p>
<p>随着2025年即将结束，我想回顾今年大型语言模型中一些最重要的发展，反思那些仍然存在的局限性和未解决的问题，并分享一些对未来可能出现的展望。</p>
<h1 id="1-推理之年、RLVR-和-GRPO"><a href="#1-推理之年、RLVR-和-GRPO" class="headerlink" title="1. 推理之年、RLVR 和 GRPO"></a>1. 推理之年、RLVR 和 GRPO</h1><p>缩放依然有效，但实际上并没有改变大型语言模型的行为或实际体验（唯一的例外是 OpenAI 新发布的 o1，它增加了推理痕迹）。因此，当 DeepSeek 在 2025 年 1 月发布他们的 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2501.12948">R1 论文</a> ，证明通过强化学习可以发展类推理行为时，这成为了一件大事。（在大型语言模型（LLM）中，推理意味着模型解释其答案，而这种解释本身通常会提高答案的准确性。）</p>
<h2 id="1-1-The-DeepSeek-Moment"><a href="#1-1-The-DeepSeek-Moment" class="headerlink" title="1.1 The DeepSeek Moment"></a><strong>1.1 The DeepSeek Moment</strong></h2><p>DeepSeek R1 因多种原因备受关注：</p>
<p>首先，DeepSeek R1 作为一个开权重模型发布，表现非常好，可与当时最好的专有模型（ChatGPT、Gemini 等）相媲美。</p>
<p>其次，DeepSeek R1 论文促使许多人，尤其是投资者和记者，重新审视了 2024 年 12 月的 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2412.19437">DeepSeek V3 早期论文</a> 。这导致了一个修正的结论：虽然训练最先进的模型仍然昂贵，但可能比之前假设便宜一个数量级，估计成本接近 500 万美元，而非 500 万或 5 亿美元。DeepSeek R1 <a target="_blank" rel="noopener" href="https://static-content.springer.com/esm/art%3A10.1038%2Fs41586-025-09422-z/MediaObjects/41586_2025_9422_MOESM1_ESM.pdf">补充材料</a>估计，在 DeepSeek V3 基础上训练 DeepSeek R1 模型又需要额外花费 294,000 美元，这远低于所有人的预期。</p>
<p>第三，也是最有趣的是，论文提出了_带有可验证奖励的强化学习_ （RLVR）与 GRPO 算法，作为一种新的（或至少是修改的）算法方法，用于开发所谓的推理模型并在训练后改进 LLMs。[[RLHF（Reinforcement Learning from Human Feedback，基于人类反馈的强化学习）|RLHF（Reinforcement Learning from Human Feedback，基于人类反馈的强化学习）]]<br>![[ObsidianPicture&#x2F;Pasted image 20260102103717.png]]</p>
<p>到目前为止，像监督式指令微调（SFT）和人类反馈强化学习（RLHF）等培训后方法，仍然是培训流程的重要组成部分，但由于需要昂贵的书面回应或偏好标签，这些方法都受到瓶颈。（当然，也可以用其他大型语言模型合成生成，但这有点像先有蛋的问题。）</p>
<p>DeepSeek R1 和 RLVR 的重要之处在于，它们允许我们对大量数据进行后训练，这使它们成为通过后训练时（在可用计算预算下）提升和解锁能力的优秀候选。</p>
<p>RLVR 中的 V 代表“可验证”，这意味着我们可以使用确定性方法来分配正确性标签，这些标签足以让大型语言模型学习复杂问题解决。（典型类别是数学和代码，但也可以将这一思想扩展到其他领域。）</p>
<p>话虽如此，结论是今年的 LLM 开发基本上被使用 RLVR 和 GRPO 的推理模型主导。</p>
<p>基本上，每个主要的开放权重或专有 LLM 开发者都发布了基于 DeepSeek R1 的推理（通常称为“思考”）模型变体。</p>
<h2 id="1-2-LLM-Focus-Points"><a href="#1-2-LLM-Focus-Points" class="headerlink" title="1.2 LLM Focus Points"></a><strong>1.2 LLM Focus Points</strong></h2><p>如果我要简明扼要地总结每年 LLM 开发重点，除了仅仅扩展架构和预训练计算，我的清单会是这样：</p>
<ul>
<li><p><strong>2022 RLHF + PPO  2022 年 RLHF+PPO</strong></p>
</li>
<li><p><strong>2023 LoRA SFT  2023 年 LoRA 系列赛</strong></p>
</li>
<li><p><strong>2024 Mid-Training  2024年中期训练</strong></p>
</li>
<li><p><strong>2025 RLVR + GRPO  2025 年 RLVR+GRPO</strong></p>
</li>
</ul>
<p>预培训仍然是一切的必要基础。除此之外，RLHF（通过 PPO 算法）当然也是 2022 年我们最初出现 ChatGPT 模型的关键。</p>
<p>2023 年，人们非常关注 LoRA 及其类似 LoRA 的参数高效微调技术，用于训练小型定制 LLM。</p>
<p>随后，在2024年，所有主要实验室开始通过专注于合成数据、优化数据组合、使用领域特定数据以及增加专门的长上下文训练阶段，使其（预）训练流程更加复杂。我在2024年的文章中总结了这些不同的方法（我把这些技术归为预训练，因为当时“中期训练”这个词还没被创造出来）：</p>
<p>当时，我把这些看作预训练技术，因为它们使用相同的预训练算法和目标。如今，这些稍为专业化的预培训阶段，紧随常规预培训，基于一般数据，通常被称为“中期培训”（作为常规预培训与后培训之间的桥梁，后期培训包括 SFT、RLHF 以及现在的 RLVR）。</p>
<p>我认为明年我们会看到（甚至）更多关注 RLVR。目前，RLVR 主要应用于数学和代码领域。下一步是不仅将最终答案的正确性作为奖励信号，还要在 RLVR 训练中评判 LLM 的解释。这在过去很多年里就已经有人在研究名称下进行过，称为“过程奖励模型”（PRMs）。不过，目前还没有特别成功。</p>
<p>不过，看看上个月发布的 DeepSeekMath-V2 论文，我在之前的文章 《 <a target="_blank" rel="noopener" href="https://magazine.sebastianraschka.com/p/technical-deepseek)">从 DeepSeek V3 到 V3.2：架构、稀疏注意力与强化学习更新</a> 》中讨论过，我认为未来我们会看到更多“解释评分”作为训练信号。</p>
<p>目前解释的评分方式涉及第二个大型语言模型。这引出了我看到的 RLVR 的另一个方向：向数学和编程以外的其他领域扩展。</p>
<p>所以，如果你今天问我2026年和2027年我对未来的看法，我会说：</p>
<ul>
<li><p><strong>2026 RLVR extensions and more inference-time scaling<br>  2026 年 RLVR 扩展及更多推理时间尺度</strong></p>
</li>
<li><p><strong>2027 Continual learning  2027年持续学习</strong></p>
</li>
</ul>
<p>推理扩展并不是新范式，大型语言模型平台已经在底层使用了一些技术。这是延迟、成本和响应准确性之间的权衡。然而，在某些应用中，准确性比延迟和成本更重要，极端的推理缩放完全值得。例如，正如最近的 <a target="_blank" rel="noopener" href="https://arxiv.org/html/2511.22570v1">DeepSeekV2-Math 论文</a>所示，它将模型推向了挑战数学竞赛基准测试中的金牌表现。</p>
<p>今年同事们也在讨论持续学习。简而言之，持续学习就是用新数据或知识训练模型，而不是从零开始重新训练。这并不是新想法，我很好奇为什么今年它被提得这么多，因为到目前为止，持续学习方面还没有任何新的或实质性的突破。持续学习的挑战是灾难性的遗忘（正如持续预训练实验所示，学习新知识意味着 LLM 在某种程度上会遗忘旧知识）。不过，既然这话题如此热门，我预计未来几年会在减少灾难性遗忘和使持续学习方法开发成为重要发展方面取得更多进展。</p>
<h1 id="2-GRPO，年度研究宠儿"><a href="#2-GRPO，年度研究宠儿" class="headerlink" title="2. GRPO，年度研究宠儿"></a>2. GRPO，年度研究宠儿</h1><p>[[GRPO（Group Relative Policy Optimization，分组相对策略优化）|GRPO（Group Relative Policy Optimization，分组相对策略优化）]]<br>在昂贵的大型语言模型时代，近年来学术研究变得有些挑战。当然，那些成为主流且成为 LLM 进展和突破的关键支柱的重要发现，即使（或正因）预算较少，学术界也能实现。</p>
<p>近年来，流行的例子包括 LoRA（<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2106.09685">LoRA：大型语言模型的低阶适应</a> 2021）及相关参数高效微调方法。</p>
<p>另一个是 DPO（ <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2305.18290">直接偏好优化：你的语言模型其实是奖励模型</a> ）及其相关的无奖励模型对齐方法，作为一种基于人类反馈的替代强化学习。</p>
<p>今年的研究亮点是 GRPO。虽然它是在 DeepSeek R1 论文中提出的，而非学术界的发端，但它依然为研究人员带来了激动人心的一年：RLVR 和 GRPO 在概念上都很有趣，且根据规模不同，实验成本并不高昂。在 LLM 研究文献中看到了许多 GRPO 的数学改进（包括公司和学术研究者），这些改进后来被采用进了最先进的 LLM 的培训流程。</p>
<h1 id="3-LLM-架构：分岔路口"><a href="#3-LLM-架构：分岔路口" class="headerlink" title="3. LLM 架构：分岔路口"></a>3. LLM 架构：分岔路口</h1><p>在大型语言模型架构中，最先进的模型仍然使用传统的解码器式变换器。然而，今年开权重大型语言模型基本趋向于使用专家混合层（MoE），以及至少一种“效率调整”的注意力机制：分组查询注意力、滑动窗口注意力或多头潜在注意力。</p>
<p>除了这些相当标准的大型语言模型架构外，我们还看到更为激进的效率调整，针对注意力机制以随序列长度线性扩展。例如 Qwen3-Next 和 Kimi Linear 中的门控 DeltaNet，以及 NVIDIA Nemotron 3 中的 Mamba-2 层</p>
<p>总之，我不想在这里讲得太详细，因为如果你想了解更多，我有一篇 1.3 万字且最近更新的文章专门讲这些架构：《大型语言模型架构比较》，<a target="_blank" rel="noopener" href="https://magazine.sebastianraschka.com/p/the-big-llm-architecture-comparison">https://magazine.sebastianraschka.com/p/the-big-llm-architecture-comparison</a></p>
<p>我的预测是，我们至少还会继续建设，并且在transformer架构上至少还会有几年，至少在最先进的建模性能方面是这样</p>
<p>同时，我确实认为我们会看到越来越多的效率和工程调整，比如门控 DeltaNet 和 Mamba 层，因为在 LLM 的训练、部署和使用规模上，从财务角度来看，这对这些仍在为 LLM 服务花费大量资金的公司来说是合理的。</p>
<p>这并不意味着没有其他选择。正如我在 <a target="_blank" rel="noopener" href="https://magazine.sebastianraschka.com/p/beyond-standard-llms">《超越标准大语言模型</a> 》中写过的， 文本扩散模型是一种有趣的方法。</p>
<h1 id="4-这也是推理尺度与工具使用的年份"><a href="#4-这也是推理尺度与工具使用的年份" class="headerlink" title="4. 这也是推理尺度与工具使用的年份"></a>4. 这也是推理尺度与工具使用的年份</h1><p>通过扩展训练数据和架构来改进大型语言模型已成为一个既定的公式，并且（至今）仍在不断带来成果。不过，尤其是今年，它已经不再是“唯一”足够的食谱了</p>
<p>我们在 GPT 4.5（2025 年 2 月）中看到了这一点，据传它比 GPT 4（以及后来发布的 GPT 5）大得多，单纯的缩放通常并不是最明智的做法。GPT 4.5 的能力可能优于 GPT 4，但增加的训练预算被认为是“性价比不高”。相反，更好的培训流程（更注重中期和后培训）和推理扩展推动了今年的大部分进展。</p>
<p>另一个重大进步来自于训练 LLM 时考虑使用工具。正如你可能知道的，幻觉是大型语言模型最大的问题之一。可以说，幻觉率一直在改善，我认为这很大程度上归功于这些工具的使用。例如，当被问及 1998 年国际足联足球世界杯冠军时，大语言模型不依赖记忆，而是通过传统搜索引擎工具，从相关可信网站（例如，这里指官方国际足联网站）中选择并抓取这些信息。数学题、计算器 API 等也是如此。</p>
<p>遗憾的是，开源生态系统尚未完全跟上这一趋势，许多甚至大多数工具仍然默认运行这些大型语言模型为非工具使用模式。其中一个原因是这是一个更新且不断演变的范式，工具需要相应调整。另一个原因是，这个问题更难解决，主要是因为安全性问题（允许大型语言模型无限制使用工具可能存在安全风险，或者对系统造成其他破坏）。我认为最合理的问题是：你会信任一个新实习生在你系统上有这么多访问权限的情况下做这件事吗？）</p>
<p><strong>我确实认为，在未来几年，本地使用 LLM 时，启用和允许使用工具将变得越来越普遍。</strong></p>
<h1 id="5-年度词汇：Benchmaxxing"><a href="#5-年度词汇：Benchmaxxing" class="headerlink" title="5. 年度词汇：Benchmaxxing"></a>5. 年度词汇：Benchmaxxing</h1><p>这里的 benchmaxxing 意味着高度关注排行榜数据，有时甚至将基准测试性能本身作为目标，而非整体能力的代理指标。</p>
<p>一个显著的例子是 Llama 4，它在许多既定基准测试中得分极高。然而，用户和开发者一旦接触到它，就会发现这些分数并不能反映实际的实际能力和实用性。</p>
<p>俗话说，如果测试集是公开的，那就不是真正的测试集。而现在的问题是，测试集数据不仅是训练语料库的一部分（无论是有意还是无意的），而且在 LLM 开发过程中经常被直接优化。</p>
<p>在 LLM 开发中，基准数据已不再是衡量 LLM 性能的可靠指标。</p>
<p>不过，我确实认为基准测试仍然是大型语言模型必须跨越的必要门槛。也就是说，如果我看到一个大型语言模型在基准测试 Y 中得分低于 X，我就已经知道它不是个好 LLM。然而，如果它在基准测试 Y 上得分高于 X，并不意味着它比另一个在同一基准测试中得分高于 X 的 LLM 好多少</p>
<p>另一个需要考虑的方面是，图像分类器只有一个任务，即对图像进行分类。然而，LLM 被用于许多不同的任务：文本翻译、摘要、编写代码、头脑风暴、解决数学问题等等。在有明确指标如分类准确率的情况下，评估图像分类器比在确定性和自由形式任务上评估 LLM 要简单得多。</p>
<p>顺便说一句，如果你想了解更多关于 LLM 评估的主要类别，你可能会喜欢我的文章《从零开始理解 LLM 评估的四大主要方法》[[从零开始理解 LLM 评估的四大主要方法]]</p>
<h1 id="6-人工智能用于编码、写作和研究"><a href="#6-人工智能用于编码、写作和研究" class="headerlink" title="6. 人工智能用于编码、写作和研究"></a>6. 人工智能用于编码、写作和研究</h1><h1 id="7-边缘：私人数据"><a href="#7-边缘：私人数据" class="headerlink" title="7. 边缘：私人数据"></a>7. 边缘：私人数据</h1><p>LLM 的一般编码、知识解答和写作能力不断提升。这在很大程度上是正确的，因为扩展性依然能带来正向的投资回报，这得益于训练流程和范式（例如 RLVR）以及推理扩展和工具使用的改进。然而，除非我们继续发明新的训练方法和&#x2F;或架构（目前没人知道这些方法会是什么样子），否则这一趋势迟早会趋于平稳（类似于我们看到的 GPT 4 到 GPT 4.5 开发阶段）。LLM 目前能够解决很多通用任务和较容易完成的任务。但要让他们在某些行业扎根，就需要更多的领域专精。我认为 LLM 提供商会很想获得高质量、领域特定的数据。目前来看，这将是一个挑战。</p>
<p>目前，大型语言模型的开发成本高昂且规模化挑战巨大，这也是为什么只有少数大型公司开发最先进的大型语言模型。不过，我认为 LLM 开发正变得越来越商品化，因为 LLM 开发者经常在不同雇主之间轮换，最终会被大型金融机构、生物技术公司以及其他有预算的公司聘用，开发具有竞争力的内部 LLMs，这些 LLM 能利用他们的私有数据</p>
<h1 id="8-2025年的惊喜"><a href="#8-2025年的惊喜" class="headerlink" title="8. 2025年的惊喜"></a>8. 2025年的惊喜</h1><ol>
<li>已有多个推理模型在主要数学竞赛中达到<a target="_blank" rel="noopener" href="https://www.nature.com/articles/d41586-025-02343-x">金牌级表现</a> （OpenAI 的未命名模型、<a target="_blank" rel="noopener" href="https://deepmind.google/blog/advanced-version-of-gemini-with-deep-think-officially-achieves-gold-medal-standard-at-the-international-mathematical-olympiad/">Gemini Deep Think</a> 和开权重的 <a target="_blank" rel="noopener" href="https://arxiv.org/html/2511.22570v1">DeepSeekMath-V2</a>）。我并不惊讶这种情况发生在整体上，但我很惊讶这事已经发生在 2025 年，而不是 2026 年。</li>
<li>Llama 4（或称 Llama）在开放权重社区中几乎完全失宠，而 Qwen 则在受欢迎程度上超过了 Llama（根据 <a target="_blank" rel="noopener" href="https://open.substack.com/users/10472909-nathan-lambert?utm_source=mentions">Nathan Lambert</a> 的 <a target="_blank" rel="noopener" href="https://www.atomproject.ai/">ATOM 项目</a>报告的下载和衍生品数量 ）。</li>
<li>Mistral AI 在 2025 年 12 月宣布的最新旗舰 Mistral 3 型号<a target="_blank" rel="noopener" href="https://magazine.sebastianraschka.com/i/168650848/mistral">中采用了 DeepSeek V3</a> 架构。</li>
<li>除了 Qwen3 和 DeepSeek R1&#x2F;V3.2 外，许多其他竞争者也在争夺开放重量级最先进车型的竞争中涌现，包括 Kimi、GLM、MiniMax 和Yi</li>
<li>更便宜、更高效的混合架构已经成为领先实验室（如 <a target="_blank" rel="noopener" href="https://magazine.sebastianraschka.com/i/168650848/qwen-next">Qwen3-Next</a>、<a target="_blank" rel="noopener" href="https://magazine.sebastianraschka.com/i/168650848/kimi-linear">Kimi Linear</a>、<a target="_blank" rel="noopener" href="https://magazine.sebastianraschka.com/p/the-big-llm-architecture-comparison">Nemotron 3</a>）中优先考虑的重点 ，而不是由独立实验室开发。</li>
<li>OpenAI 发布了一个开放权重模型（gpt-oss），我今年早些时候写过一篇独立<a target="_blank" rel="noopener" href="https://magazine.sebastianraschka.com/p/from-gpt-2-to-gpt-oss-analyzing-the">文章</a>介绍它。</li>
<li><a target="_blank" rel="noopener" href="https://modelcontextprotocol.io/docs/getting-started/intro">MCP</a>（ <a target="_blank" rel="noopener" href="https://www.linuxfoundation.org/press/linux-foundation-announces-the-formation-of-the-agentic-ai-foundation">加入 Linux 基金会</a> ）已经成为代理风格 LLM 系统中工具和数据访问的标准（目前如此）;我预计 2025 年生态系统会更加分散，至少直到 2026 年。</li>
</ol>
<h1 id="9-2026年预测"><a href="#9-2026年预测" class="headerlink" title="9 2026年预测"></a><strong>9 2026年预测</strong></h1><ol>
<li>我们很可能会看到一个面向消费者的行业规模扩散模型，实现廉价、可靠、低延迟的推断，Gemini Diffusion 很可能率先出现。</li>
<li>开放权重社区将缓慢但稳步地采用具有本地工具使用和日益代理能力的大型语言模型。</li>
<li>RLVR 将更广泛地扩展到数学和编程以外的其他领域（例如化学、生物等）。</li>
<li>经典的 RAG 将逐渐淡出文档查询的默认解决方案。开发者将不再对每个文档相关查询都使用检索，而是更多依赖更好的长上下文处理，尤其是随着更多“小”开权重模型的出现。</li>
<li>大量大型语言模型基准和性能提升将来自改进的工具和推理时间扩展，而非训练或核心模型本身。看起来 LLM 变得越来越好，但这主要是因为周边应用在不断进步。与此同时，开发者将更多地关注降低延迟，使推理模型在不必要的推理标记上扩展更少。别误会，2026 年会推动最先进的技术，但今年的进步更多来自推断，而不仅仅是训练方面。</li>
</ol>
<p>总结来说，如果说2025年有一个元教训，那就是大型语言模型的进步不再依赖单一突破，而是通过多个独立杠杆在多个领域取得改进。这包括架构调整、数据质量改进、推理训练、推理扩展、工具调用等。</p>
<p>与此同时，评估依然艰难，基准并不完美，关于何时以及如何使用这些系统的良好判断依然至关重要。</p>
<p>我对2026年的期望是，我们能继续看到有趣的改进，同时也明白这些改进的来源。这需要更好且更一致的基准测试，当然也需要透明度。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://gaoguodong03.github.io/gdBlog">GuoDong</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://gaoguodong03.github.io/gdBlog/2026/01/02/2025%E5%B9%B4%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%8E%B0%E7%8A%B6%EF%BC%9A%E8%BF%9B%E5%B1%95%E3%80%81%E9%97%AE%E9%A2%98%E4%B8%8E%E9%A2%84%E6%B5%8B/">https://gaoguodong03.github.io/gdBlog/2026/01/02/2025%E5%B9%B4%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%8E%B0%E7%8A%B6%EF%BC%9A%E8%BF%9B%E5%B1%95%E3%80%81%E9%97%AE%E9%A2%98%E4%B8%8E%E9%A2%84%E6%B5%8B/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://gaoguodong03.github.io/gdBlog" target="_blank">果冻小配方</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/gdBlog/tags/%E8%B0%83%E7%A0%94/">调研</a><a class="post-meta__tags" href="/gdBlog/tags/LLM/">LLM</a><a class="post-meta__tags" href="/gdBlog/tags/%E6%80%BB%E7%BB%93/">总结</a><a class="post-meta__tags" href="/gdBlog/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/">强化学习</a></div><div class="post-share"><div class="social-share" data-image="/gdBlog/./img/logoLLM.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="/gdBlog/pluginsSrc/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="/gdBlog/pluginsSrc/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/gdBlog/2026/01/02/GRPO%EF%BC%88Group-Relative-Policy-Optimization%EF%BC%8C%E5%88%86%E7%BB%84%E7%9B%B8%E5%AF%B9%E7%AD%96%E7%95%A5%E4%BC%98%E5%8C%96%EF%BC%89/" title="GRPO（Group Relative Policy Optimization，分组相对策略优化）"><img class="cover" src="/gdBlog/./img/logoLLM.png" onerror="onerror=null;src='/gdBlog/img/404.jpg'" alt="cover of previous post"><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">GRPO（Group Relative Policy Optimization，分组相对策略优化）</div></div><div class="info-2"><div class="info-item-1">GRPO（Group Relative Policy Optimization，分组相对策略优化） 是 DeepSeek 在 V3 模型训练中引入的一种无奖励模型的强化学习对齐技术。它是一种 RLHF（人类反馈强化学习）的变体或改进方法，旨在更高效、更稳定地训练模型与人类偏好对齐。[[RLHF（Reinforcement Learning from Human Feedback，基于人类反馈的强化学习）|RLHF（Reinforcement Learning from Human Feedback，基于人类反馈的强化学习）]] 核心思路GRPO 的核心创新在于摆脱了对独立奖励模型的依赖。传统的 RLHF 需要先训练一个复杂的奖励模型来评估生成内容，而 GRPO 直接利用人类偏好数据进行端到端的策略优化。 工作原理（简化版） 分组对比：  对于同一个问题（提示），模型会生成一组（例如 4 个）不同的回答。 这些回答根据质量被模型或参考标准排序。   相对偏好建模：  GRPO 的核心是直接比较同一组内回答的相对好坏，而不是像传统 RLHF 那样依赖一个绝对分数预测的奖励模型。 它通...</div></div></div></a><a class="pagination-related" href="/gdBlog/2026/01/01/RSS%EF%BC%88Really-Simple-Syndication%EF%BC%88%E7%AE%80%E6%98%93%E4%BF%A1%E6%81%AF%E8%81%9A%E5%90%88%EF%BC%89%EF%BC%89/" title="RSS（Really Simple Syndication（简易信息聚合））"><div class="cover" style="background: ./img/"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">RSS（Really Simple Syndication（简易信息聚合））</div></div><div class="info-2"><div class="info-item-1">订阅方法folo 一、最简单的理解：把它想象成你的“信息管家”想象一下，你喜欢阅读很多不同来源的内容：  A 网站的新闻 B 博客的技术文章 C 播客（Podcast）的最新节目 D 论坛的特定板块  通常，你需要分别打开这些网站或 App，像逛菜市场一样，一个一个摊位看过去，才能知道有没有新东西。这很浪费时间，而且容易分心。 而 RSS 就像你雇佣的一个专属信息管家。 你只需要告诉他：“请帮我订阅 A、B、C、D 这几个地方的内容。” 之后，每当这些地方有任何更新，你的管家就会自动把最新的内容（通常是标题、摘要和链接）收集起来，整理好放在一个地方。你每天只需要打开这个“管家 App”（也就是 RSS 阅读器），就能在一个清爽的界面里，看到所有你关心的更新，再也不用亲自跑腿了。 核心转变：从“人找信息”变成了“信息找人”。  二、正式的解释：RSS 是什么？RSS 的全称是 Really Simple Syndication（简易信息聚合）。它是一种信息发布的格式规范，本质上是一个包含了网站最新内容更新的 .xml 格式文件。  发布方（网站&#x2F;博客）：会生成一个遵循 R...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/gdBlog/2026/01/02/GRPO%EF%BC%88Group-Relative-Policy-Optimization%EF%BC%8C%E5%88%86%E7%BB%84%E7%9B%B8%E5%AF%B9%E7%AD%96%E7%95%A5%E4%BC%98%E5%8C%96%EF%BC%89/" title="GRPO（Group Relative Policy Optimization，分组相对策略优化）"><img class="cover" src="/gdBlog/./img/logoLLM.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2026-01-02</div><div class="info-item-2">GRPO（Group Relative Policy Optimization，分组相对策略优化）</div></div><div class="info-2"><div class="info-item-1">GRPO（Group Relative Policy Optimization，分组相对策略优化） 是 DeepSeek 在 V3 模型训练中引入的一种无奖励模型的强化学习对齐技术。它是一种 RLHF（人类反馈强化学习）的变体或改进方法，旨在更高效、更稳定地训练模型与人类偏好对齐。[[RLHF（Reinforcement Learning from Human Feedback，基于人类反馈的强化学习）|RLHF（Reinforcement Learning from Human Feedback，基于人类反馈的强化学习）]] 核心思路GRPO 的核心创新在于摆脱了对独立奖励模型的依赖。传统的 RLHF 需要先训练一个复杂的奖励模型来评估生成内容，而 GRPO 直接利用人类偏好数据进行端到端的策略优化。 工作原理（简化版） 分组对比：  对于同一个问题（提示），模型会生成一组（例如 4 个）不同的回答。 这些回答根据质量被模型或参考标准排序。   相对偏好建模：  GRPO 的核心是直接比较同一组内回答的相对好坏，而不是像传统 RLHF 那样依赖一个绝对分数预测的奖励模型。 它通...</div></div></div></a><a class="pagination-related" href="/gdBlog/2025/10/01/DeepSearch%E7%9A%84%E8%AE%BA%E6%96%87%E8%B0%83%E7%A0%94/" title="DeepSearch的论文调研"><img class="cover" src="/gdBlog/./img/logoZiyouzhiyi.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-10-01</div><div class="info-item-2">DeepSearch的论文调研</div></div><div class="info-2"><div class="info-item-1">论文调研综述论文A Comprehensive Survey of Deep Research: Systems, Methodologies, and Applications该综述由浙江大学徐仁军和彭静雯撰写，系统研究了Deep Research 系统这一快速发展的领域 —— 这类系统通过整合大型语言模型（LLMs）、先进信息检索技术和自主推理能力，实现复杂研究工作流的自动化；综述分析了 2023 年以来出现的80 多个商业和非商业系统（如 OpenAI&#x2F;DeepResearch、Gemini&#x2F;DeepResearch 等），提出了基于 “基础模型与推理引擎、工具利用与环境交互、任务规划与执行控制、知识合成与输出生成” 的四层技术分类体系，探讨了系统在学术、科学、商业、教育等领域的架构模式与应用适配性，指出当前系统在信息准确性、隐私、知识产权等方面的技术与伦理挑战，并明确了先进推理架构、多模态融合、领域专业化等未来研究方向，同时提供了相关资源库（https://github.com/scienceaix/deepresearch）支持进一步研究 ![[Ob...</div></div></div></a><a class="pagination-related" href="/gdBlog/2026/01/02/RLHF%EF%BC%88Reinforcement-Learning-from-Human-Feedback%EF%BC%8C%E5%9F%BA%E4%BA%8E%E4%BA%BA%E7%B1%BB%E5%8F%8D%E9%A6%88%E7%9A%84%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%89/" title="RLHF（Reinforcement Learning from Human Feedback，基于人类反馈的强化学习）"><div class="cover" style="background: ./img/"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2026-01-02</div><div class="info-item-2">RLHF（Reinforcement Learning from Human Feedback，基于人类反馈的强化学习）</div></div><div class="info-2"><div class="info-item-1">RLHF（Reinforcement Learning from Human Feedback，基于人类反馈的强化学习） 是一种用于训练机器学习模型（特别是大型语言模型）的技术方法。它的核心思想是让模型通过与人类的互动反馈来学习和优化自身行为。 主要流程RLHF 通常包含以下三个关键步骤：  监督微调（Supervised Fine-Tuning, SFT）  首先，在高质量的人类标注数据上对预训练的基础模型进行微调，使其初步学会按照人类期望的方式回答问题。   奖励模型训练（Reward Modeling, RM）  人类评估员对模型的多个输出进行排序（例如，比较两个回答哪个更好）。 根据这些偏好数据，训练一个独立的“奖励模型”（Reward Model），使其能够自动判断模型输出质量的高低（给出分数）。   强化学习微调（Reinforcement Learning Fine-Tuning）  将第一步的 SFT 模型作为初始策略模型。 使用第二步训练好的奖励模型作为“评分标准&#x2F;奖励函数”。 通过强化学习算法（如 PPO，近端策略优化）来优化策略模型，目标是最大化奖...</div></div></div></a><a class="pagination-related" href="/gdBlog/2025/10/11/%E4%B8%8A%E4%B8%8B%E6%96%87%E5%B7%A5%E7%A8%8B-Context-Engineering/" title="上下文工程(Context Engineering)"><img class="cover" src="/gdBlog/./img/logoZiyouzhiyi.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-10-11</div><div class="info-item-2">上下文工程(Context Engineering)</div></div><div class="info-2"><div class="info-item-1">前言随着大模型能力的快速发展，人们对大语言模型的使用方式越来越多的从简单的ChatBot，变成了各种Agent，而一个新的术语——上下文工程（Context Engineering），也逐步取代了[[提示工程]](Prompt Engineering）。 最近[Anthropic]发表了一篇文章，分享了他们在上下文工程上的探索，本文基于原文做了适当解读。 上下文工程和提示工程的区别—记忆简单来说，提示工程重点关注的是为了获得最佳结果而编写和组织的LLM指令的方法，各种在公众号、B站、小红书上所介绍的如何在某一款软件中输入特定的指令从而能够获得某种效果的方法，都属于提示工程，它通常涉及需要完成的任务、任务的背景信息、如何更好的完成任务、哪些是禁止的、受众是谁等，例如如何使用AI写文章、如何去除AI味、如何使用豆包生成特定的图片等。 而上下文工程指的是，在LLM推理过程中，如何管理和维护要输入给LLM的最佳信息的策略集，之所以叫信息，而没有叫Prompt或者提示语，是因为在上下文工程中，除了涉及给LLM安排任务，还涉及到工具、当前任务所需要的参考资料、之前交互过程中的记忆等，参考资料...</div></div></div></a><a class="pagination-related" href="/gdBlog/2025/10/11/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%8D%8F%E5%90%8C%E6%9E%B6%E6%9E%84%E8%B0%83%E7%A0%94/" title="多智能体协同架构调研"><img class="cover" src="/gdBlog/./img/logoZiyouzhiyi.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-10-11</div><div class="info-item-2">多智能体协同架构调研</div></div><div class="info-2"><div class="info-item-1">上下文工程 任务管理者 依次往总线扔任务 会议时间文档链接：https://free4inno.feishu.cn/wiki/CxV0wuVATi8Z0hk6wF8cWgzCnTb报告时间：2025&#x2F;10&#x2F;10报告人：孟千斌 简要记录调研了多种多智能体协同架构，包括 OpenManus、微软开源的 AutoGen、字节开源的 LongManus（基于 LongGraph）等，并对它们进行了横向对比  OpenManus：因闭源，分析基于官网 Blog 及演示视频，聚焦其多智能体协同模式，单智能体模式未涉及。 AutoGen：提供 5 种多智能体协同解法： 双智能体聊天（带函数调用）：由用户代理和助手构成，通过交替对话、任务拆分函数调用逐步完成任务，支持人工在关键节点介入。 群聊（Group chat）：预设多角色智能体与群聊管理器，管理器根据上下文和角色能力自动路由任务，公共历史记录信息，支持发布 &#x2F; 订阅模式优化。 AutoBuild：能从自然语言需求出发，自动生成带角色定位的智能体并组成群聊，经协作完成任务后反思总结并返回结果。 Mixture ...</div></div></div></a><a class="pagination-related" href="/gdBlog/2026/01/02/%E6%8F%90%E7%A4%BA%E5%B7%A5%E7%A8%8B%E8%B0%83%E7%A0%94/" title="提示工程调研"><img class="cover" src="/gdBlog/./img/logoZiyouzhiyi.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2026-01-02</div><div class="info-item-2">提示工程调研</div></div><div class="info-2"><div class="info-item-1">写在前面  文档链接 open ai 持续强化 ChatGPT 图谱以抵御即时注入攻击 [[..&#x2F;img&#x2F;logoZiyouzhiyi.jpg]][[基于提示词工程的通用大模型]] 参考文献提示工程指南 | Prompt Engineering Guide Google 《Prompt Engineering提示词手册》Prompt Engineering_v7.pdf 综述：大语言模型 THE CHINESE BOOK FOR LARGE LANGUAGE MODELS 内容速览一些编写提示词的建议 一些提示技术和思想 快速使用–好用的提示词框架—-&gt;****提示工程调研 一些值得思考的问题 提示工程简介定义：针对特定任务设计合适的任务提示,这一过程被称为提示工程。 提示工程(Prompt Engineering)是一门相对较新的学科，伴随大语言模型快速发展而兴起。提示工程不仅仅是关于设计提示词，它包含了与大语言模型交互和研发的各种技能和技术。 出现原因 同一个模型，通过不同的提示方式，效果差异巨大  低成本、快速迭代、无需训练、对所有人开放   提示...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/gdBlog/img/touxiang.jpg" onerror="this.onerror=null;this.src='/gdBlog/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">GuoDong</div><div class="author-info-description">碎碎念念 岁岁年年</div><div class="site-data"><a href="/gdBlog/archives/"><div class="headline">文章</div><div class="length-num">29</div></a><a href="/gdBlog/tags/"><div class="headline">标签</div><div class="length-num">20</div></a><a href="/gdBlog/categories/"><div class="headline">分类</div><div class="length-num">7</div></a></div><div class="card-info-social-icons"><a class="social-icon" href="https://www.douyin.com/" target="_blank" title="douyin"><i class="fab fa-tiktok" style="color: #24292e;"></i></a><a class="social-icon" href="https://www.bilibili.com/" target="_blank" title="bilibili"><i class="fab fa-bilibili" style="color: #24292e;"></i></a><a class="social-icon" href="https://github.com/gaoguodong03" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="https://leetcode.cn/studyplan/top-100-liked/" target="_blank" title="LeetCode"><i class="fab fa-comments" style="color: #24292e;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">有重要的事情 得去趟南京<br>苦苦刷LeetCode<br>常驻实验室选手<br>健身房小小白</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#1-%E6%8E%A8%E7%90%86%E4%B9%8B%E5%B9%B4%E3%80%81RLVR-%E5%92%8C-GRPO"><span class="toc-number">1.</span> <span class="toc-text">1. 推理之年、RLVR 和 GRPO</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-1-The-DeepSeek-Moment"><span class="toc-number">1.1.</span> <span class="toc-text">1.1 The DeepSeek Moment</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-2-LLM-Focus-Points"><span class="toc-number">1.2.</span> <span class="toc-text">1.2 LLM Focus Points</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2-GRPO%EF%BC%8C%E5%B9%B4%E5%BA%A6%E7%A0%94%E7%A9%B6%E5%AE%A0%E5%84%BF"><span class="toc-number">2.</span> <span class="toc-text">2. GRPO，年度研究宠儿</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#3-LLM-%E6%9E%B6%E6%9E%84%EF%BC%9A%E5%88%86%E5%B2%94%E8%B7%AF%E5%8F%A3"><span class="toc-number">3.</span> <span class="toc-text">3. LLM 架构：分岔路口</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#4-%E8%BF%99%E4%B9%9F%E6%98%AF%E6%8E%A8%E7%90%86%E5%B0%BA%E5%BA%A6%E4%B8%8E%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8%E7%9A%84%E5%B9%B4%E4%BB%BD"><span class="toc-number">4.</span> <span class="toc-text">4. 这也是推理尺度与工具使用的年份</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#5-%E5%B9%B4%E5%BA%A6%E8%AF%8D%E6%B1%87%EF%BC%9ABenchmaxxing"><span class="toc-number">5.</span> <span class="toc-text">5. 年度词汇：Benchmaxxing</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#6-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E7%94%A8%E4%BA%8E%E7%BC%96%E7%A0%81%E3%80%81%E5%86%99%E4%BD%9C%E5%92%8C%E7%A0%94%E7%A9%B6"><span class="toc-number">6.</span> <span class="toc-text">6. 人工智能用于编码、写作和研究</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#7-%E8%BE%B9%E7%BC%98%EF%BC%9A%E7%A7%81%E4%BA%BA%E6%95%B0%E6%8D%AE"><span class="toc-number">7.</span> <span class="toc-text">7. 边缘：私人数据</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#8-2025%E5%B9%B4%E7%9A%84%E6%83%8A%E5%96%9C"><span class="toc-number">8.</span> <span class="toc-text">8. 2025年的惊喜</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#9-2026%E5%B9%B4%E9%A2%84%E6%B5%8B"><span class="toc-number">9.</span> <span class="toc-text">9 2026年预测</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/gdBlog/2026/01/02/2025%E5%B9%B4%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%8E%B0%E7%8A%B6%EF%BC%9A%E8%BF%9B%E5%B1%95%E3%80%81%E9%97%AE%E9%A2%98%E4%B8%8E%E9%A2%84%E6%B5%8B/" title="2025年大型语言模型现状：进展、问题与预测">2025年大型语言模型现状：进展、问题与预测</a><time datetime="2026-01-01T16:00:00.000Z" title="发表于 2026-01-02 00:00:00">2026-01-02</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/gdBlog/2026/01/02/GRPO%EF%BC%88Group-Relative-Policy-Optimization%EF%BC%8C%E5%88%86%E7%BB%84%E7%9B%B8%E5%AF%B9%E7%AD%96%E7%95%A5%E4%BC%98%E5%8C%96%EF%BC%89/" title="GRPO（Group Relative Policy Optimization，分组相对策略优化）">GRPO（Group Relative Policy Optimization，分组相对策略优化）</a><time datetime="2026-01-01T16:00:00.000Z" title="发表于 2026-01-02 00:00:00">2026-01-02</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/gdBlog/2026/01/02/RLHF%EF%BC%88Reinforcement-Learning-from-Human-Feedback%EF%BC%8C%E5%9F%BA%E4%BA%8E%E4%BA%BA%E7%B1%BB%E5%8F%8D%E9%A6%88%E7%9A%84%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%89/" title="RLHF（Reinforcement Learning from Human Feedback，基于人类反馈的强化学习）">RLHF（Reinforcement Learning from Human Feedback，基于人类反馈的强化学习）</a><time datetime="2026-01-01T16:00:00.000Z" title="发表于 2026-01-02 00:00:00">2026-01-02</time></div></div></div></div></div></div></main><footer id="footer" style="background: transparent;"><div class="footer-other"><div class="footer-copyright"></div><div class="footer_custom_text"><style>
  .footer {
    text-align: center;
    position: relative;
  }
  .social-links {
    display: flex;
    justify-content: center;
    gap: 1.5rem;
    flex-wrap: wrap;
  }
  .social-links i {
    color: #000000;
  }
  .social-link {
    color: #000000;
    font-size: 1.2rem;
    transition: all 0.3s ease;
    display: inline-flex;
    align-items: center;
    justify-content: center;
    width: 2.5rem;
    height: 2.5rem;
    border-radius: 50%;
    background: rgba(0, 0, 0, 0.1);
  }
  .social-link:hover {
    color: #333333;
    background: rgba(0, 0, 0, 0.2);
    transform: translateY(-3px) scale(1.2);
    box-shadow: 0 5px 15px rgba(0, 0, 0, 0.3);
    text-decoration: none !important; 
  }
  .footer p {
    margin: 0.5rem 0;
    line-height: 1;
  }
  .copyright {
    font-size: 1.1rem; 
    color: #000000;
    font-weight: 400;
  }
  .tagline {
    font-size: 0.8rem;
    color: #333333;
    font-style: italic;
    font-weight: 500;
  }
  .visitor-count {
    font-size: 0.75rem;
    color: rgba(0, 0, 0, 0.7);
    font-weight: 300;
  }
  #visitorCount {
    font-weight: bold;
  }
</style>
<div class="footer">
  <p class="copyright">© 2025 果冻小配方 - 所有权利保留</p>
  <p class="tagline">> 碎碎念念 岁岁年年 <</p>
  <p class="visitor-count">访问量: <span id="visitorCount">1024</span> | 你是第 <span id="dailyVisitor">1</span> 位今日访客</p>
</div>
<script>
  // 确保DOM加载完成后执行
  document.addEventListener('DOMContentLoaded', function() {
    // 模拟访问量增长
    function updateVisitorCount() {
      const countElement = document.getElementById('visitorCount');
      let count = parseInt(countElement.textContent) || 1024;
      // 从localStorage获取或初始化计数
      const storedCount = localStorage.getItem('totalVisitors');
      if (storedCount) {
        count = parseInt(storedCount);
        countElement.textContent = count;
      }
      // 每日访客计数
      const today = new Date().toDateString();
      const dailyData = JSON.parse(localStorage.getItem('dailyVisitors') || '{"date":"", "count":0}');
      if (dailyData.date !== today) {
        dailyData.date = today;
        dailyData.count = 0;
      }
      dailyData.count += 1;
      document.getElementById('dailyVisitor').textContent = dailyData.count;
      localStorage.setItem('dailyVisitors', JSON.stringify(dailyData));
      // 每30秒随机增加访问量
      setInterval(() => {
        count += Math.floor(Math.random() * 3);
        countElement.textContent = count;
        localStorage.setItem('totalVisitors', count.toString());
      }, 30000);
    }
    updateVisitorCount();
    // 添加点击动画效果
    const socialLinks = document.querySelectorAll('.social-link');
    socialLinks.forEach(link => {
      link.addEventListener('click', function() {
        this.style.transform = 'scale(0.9)';
        setTimeout(() => {
          this.style.transform = '';
        }, 300);
      });
    });
  });
</script>
</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/gdBlog/js/utils.js"></script><script src="/gdBlog/js/main.js"></script><div class="js-pjax"></div><script src="/config/js/happy-title.js" async></script><script src="/config/js/foot.js" async></script></div><!-- hexo injector body_end start --><script data-pjax>
  function butterfly_categories_card_injector_config(){
    // 检查容器是否存在
    var parent_div_git = document.getElementById('recent-posts');
    // 如果容器不存在，则动态创建
    if (!parent_div_git) {
      console.warn('butterfly_categories_card: 挂载容器不存在，正在动态创建...');
      // 创建新容器（默认插入到页面主体顶部）
      parent_div_git = document.createElement('div');
      parent_div_git.id = 'recent-posts'; // 赋予配置的ID
      document.querySelector('#page').prepend(parent_div_git); // 插入到 #content-inner 内
    }
    var item_html = '<style>li.categoryBar-list-item{width:32.3%;}.categoryBar-list{max-height: 950px;overflow:auto;}.categoryBar-list::-webkit-scrollbar{width:0!important}@media screen and (max-width: 650px){.categoryBar-list{max-height: 800px;}}</style><div class="recent-post-item" style="height:auto;width:100%;padding:0px;"><div id="categoryBar"><ul class="categoryBar-list"><li class="categoryBar-list-item" style="background:url(/img/logo.jpg);"> <a class="categoryBar-list-link" href="/categories/果冻的日记本/">果冻的日记本</a><span class="categoryBar-list-count">4</span><span class="categoryBar-list-descr">果冻的航海日记</span></li><li class="categoryBar-list-item" style="background:url(/img/logo.jpg);"> <a class="categoryBar-list-link" href="/categories/果冻的理论学习/">果冻的理论学习</a><span class="categoryBar-list-count">10</span><span class="categoryBar-list-descr">Hexo</span></li><li class="categoryBar-list-item" style="background:url(/img/logo.jpg);"> <a class="categoryBar-list-link" href="/categories/果冻的奇妙小工具/">果冻的奇妙小工具</a><span class="categoryBar-list-count">5</span><span class="categoryBar-list-descr">果冻的奇妙小工具</span></li><li class="categoryBar-list-item" style="background:url(/img/logo.jpg);"> <a class="categoryBar-list-link" href="/categories/果冻的科普专区/">果冻的科普专区</a><span class="categoryBar-list-count">3</span><span class="categoryBar-list-descr">果冻的日记本</span></li><li class="categoryBar-list-item" style="background:url(undefined);"> <a class="categoryBar-list-link" href="/categories/果冻的航海日志/">果冻的航海日志</a><span class="categoryBar-list-count">5</span><span class="categoryBar-list-descr"></span></li><li class="categoryBar-list-item" style="background:url(undefined);"> <a class="categoryBar-list-link" href="/categories/Hexo/">Hexo</a><span class="categoryBar-list-count">1</span><span class="categoryBar-list-descr"></span></li><li class="categoryBar-list-item" style="background:url(undefined);"> <a class="categoryBar-list-link" href="/categories/果冻的LeetCode刷题/">果冻的LeetCode刷题</a><span class="categoryBar-list-count">5</span><span class="categoryBar-list-descr"></span></li></ul></div></div>';
    console.log('已挂载 butterfly_categories_card');
    parent_div_git.insertAdjacentHTML("afterbegin",item_html)
  }
  // 路径匹配逻辑（使用 startsWith）
  if (location.pathname.startsWith('/categories/') || '/categories/' === 'all') {
    butterfly_categories_card_injector_config();
  }
  </script><script data-pjax>
  function butterfly_swiper_injector_config(){
    var parent_div_git = document.getElementById('recent-posts');
    var item_html = '<div class="recent-post-item" style="height: auto;width: 100%"><div class="blog-slider swiper-container-fade swiper-container-horizontal" id="swiper_container"><div class="blog-slider__wrp swiper-wrapper" style="transition-duration: 0ms;"></div><div class="blog-slider__pagination swiper-pagination-clickable swiper-pagination-bullets"></div></div></div>';
    console.log('已挂载butterfly_swiper')
    parent_div_git.insertAdjacentHTML("afterbegin",item_html)
    }
  var elist = 'undefined'.split(',');
  var cpage = location.pathname;
  var epage = '/';
  var flag = 0;

  for (var i=0;i<elist.length;i++){
    if (cpage.includes(elist[i])){
      flag++;
    }
  }

  if ((epage ==='all')&&(flag == 0)){
    butterfly_swiper_injector_config();
  }
  else if (epage === cpage){
    butterfly_swiper_injector_config();
  }
  </script><script defer src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.js"></script><script defer data-pjax src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper_init.js"></script><div class="js-pjax"><script async="async">var arr = document.getElementsByClassName('recent-post-item');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__zoomIn');
    arr[i].setAttribute('data-wow-duration', '300ms');
    arr[i].setAttribute('data-wow-delay', '0ms');
    arr[i].setAttribute('data-wow-offset', '0');
    arr[i].setAttribute('data-wow-iteration', '1');
  }</script><script async="async">var arr = document.getElementsByClassName('card-widget');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__zoomIn');
    arr[i].setAttribute('data-wow-duration', '');
    arr[i].setAttribute('data-wow-delay', '');
    arr[i].setAttribute('data-wow-offset', '');
    arr[i].setAttribute('data-wow-iteration', '');
  }</script></div><script defer src="https://npm.elemecdn.com/hexo-butterfly-wowjs/lib/wow.min.js"></script><script defer src="https://npm.elemecdn.com/hexo-butterfly-wowjs/lib/wow_init.js"></script><!-- hexo injector body_end end --></body></html>