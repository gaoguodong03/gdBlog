<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>DeepSearch的论文调研</title>
    <url>/gdBlog/2025/10/01/DeepSearch/</url>
    <content><![CDATA[论文调研综述论文A Comprehensive Survey of Deep Research: Systems, Methodologies, and Applications该综述由浙江大学徐仁军和彭静雯撰写，系统研究了Deep Research 系统这一快速发展的领域 —— 这类系统通过整合大型语言模型（LLMs）、先进信息检索技术和自主推理能力，实现复杂研究工作流的自动化；综述分析了 2023 年以来出现的80 多个商业和非商业系统（如 OpenAI&#x2F;DeepResearch、Gemini&#x2F;DeepResearch 等），提出了基于 “基础模型与推理引擎、工具利用与环境交互、任务规划与执行控制、知识合成与输出生成” 的四层技术分类体系，探讨了系统在学术、科学、商业、教育等领域的架构模式与应用适配性，指出当前系统在信息准确性、隐私、知识产权等方面的技术与伦理挑战，并明确了先进推理架构、多模态融合、领域专业化等未来研究方向，同时提供了相关资源库（https://github.com/scienceaix/deepresearch）支持进一步研究
![[ObsidianPicture&#x2F;Pasted image 20251001221837.png]]
## **综述基础信息**- 作者：浙江大学 Renjun Xu、Jingwen Peng- 核心对象：Deep Research系统- 研究范围：2023年以来80+商业/非商业系统- 资源库：https://github.com/scienceaix/deepresearch## **Deep Research定义与边界**- 核心维度  - 智能知识发现：自动化文献检索、假设生成  - 端到端工作流自动化：实验设计-数据收集-分析-解读  - 协作智能增强：人机协作接口、可视化- 边界区分  - ≠通用AI助手（缺自主工作流能力）  - ≠单功能研究工具（缺跨功能整合）  - ≠纯LLM应用（缺工具集成与环境交互）## **技术框架与演进**- 基础模型与推理引擎  - 演进：通用LLM→研究专用模型（如OpenAI o3、Gemini 2.5 Pro）  - 关键能力：百万级token上下文窗口、链/树状推理- 工具利用与环境交互  - 网页交互：从API搜索→动态内容处理（如Nanobrowser）  - 内容处理：从文本提取→多模态解析（PDF、表格、可视化）  - 工具集成：从通用API→16000+专用API（ToolLLM支持）- 任务规划与执行控制  - 规划：线性分解→分层动态规划（如OpenAI/AgentsSDK）  - 执行： sequential执行→并发监控+故障恢复（如Agent-RL/ReSearch）  - 协作：单智能体→多智能体分工（如smolagents/open_deep_research）- 知识合成与输出生成  - 信息评估：源信誉→多维度质量验证（如grapeot/deep_research_agent）  - 报告生成：简单总结→结构化报告+引用（如mshumer/OpenDeepResearcher）  - 交互呈现：静态文本→动态探索界面（如HKUDS/Auto-Deep-Research）## **系统比较与评估**- 技术维度比较  - 基础模型：上下文长度（Gemini 2.5 Pro达1M tokens）、推理方式  - 环境交互：网页/API/文档/GUI支持能力  - 规划执行：任务分解、错误处理、多智能体协作  - 知识合成：源评估、输出结构、用户交互- 应用适配分析  - 学术研究：文献综述、假设生成（OpenAI/DeepResearch适配）  - 商业智能：市场分析、战略决策（Gemini/DeepResearch适配）  - 个人知识管理：信息整理、学习规划（Perplexity/DeepResearch适配）- 性能基准  - 定量：HLE（OpenAI/DeepResearch达26.6%）、GAIA（Manus达86.5%）、MMLU（Grok3Beta达92.7%）  - 定性：输出连贯性、信息多样性、验证机制## **挑战与未来方向**- 关键挑战  - 信息准确性：幻觉、事实一致性  - 隐私安全：用户数据隔离、敏感信息保护  - 知识产权：引用完整性、版权合规  - 可及性：计算资源依赖、技术门槛- 未来方向  - 先进推理：神经符号融合、因果推理、不确定性建模  - 多模态：视觉/音频/视频整合、跨模态推理  - 领域专业化：科学/法律/医疗定制优化  - 人机协作：交互工作流、标准接口、联合知识创建

一、综述基础与研究背景核心定义
Deep Research 系统是一类 AI 驱动应用，通过整合大型语言模型（LLMs）、先进信息检索技术和自主推理能力，实现复杂研究工作流的自动化，核心覆盖 “智能知识发现、端到端工作流自动化、协作智能增强” 三大维度，需与通用 AI 助手（如 ChatGPT）、单功能工具（如文献管理器）、纯 LLM 应用区分 —— 后者缺乏跨功能整合与自主工作流能力。
 研究意义
学术创新：加速假设验证（如 HotpotQA 基准任务），挖掘跨学科关联企业转型：支持大规模数据驱动决策（如 Agent-RL&#x2F;ReSearch 分析市场趋势）知识民主化：通过开源系统（如 grapeot&#x2F;deep_research_agent）降低技术门槛
二、Deep Research 系统的技术框架与演进2.1 四层技术分类体系（核心框架）


技术维度
核心能力演进
代表系统 &#x2F; 技术



基础模型与推理引擎
通用 LLM→研究专用模型；有限上下文→百万级 token 窗口；零样本推理→链 &#x2F; 树状推理
OpenAI o3（200k tokens）、Gemini 2.5 Pro（1M tokens）、AutoGLM-Research


工具利用与环境交互
API 搜索→动态网页导航；文本提取→多模态处理；通用工具→16000 + 专用 API 集成
Nanobrowser（AI 专用浏览器）、dzhng&#x2F;deep-research（多格式文档处理）、ToolLLM


任务规划与执行控制
线性任务分解→分层动态规划； sequential 执行→并发监控；单智能体→多智能体协作
OpenAI&#x2F;AgentsSDK（分层规划）、Agent-RL&#x2F;ReSearch（强化学习执行）、TARS（多智能体）


知识合成与输出生成
源信誉判断→多维度验证；简单总结→结构化报告；静态文本→动态交互呈现
grapeot&#x2F;deep_research_agent（证据评估）、mshumer&#x2F;OpenDeepResearcher（报告生成）、HKUDS&#x2F;Auto-Deep-Research（交互探索）


2.2 系统演进三阶段（2023-2025）
起源与早期探索（2023-2025.2）

技术基础：基于 n8n（工作流自动化）、AutoGPT（智能体框架）等现有工具
里程碑：2024.12 Google Gemini 推出首个 Deep Research 功能，支持基础多步推理
代表系统：cline2024（集成研究工作流）、open_operator（浏览器自动化）


技术突破与竞争（2025.2-2025.3）

关键事件：DeepSeek 开源模型（高效推理）、OpenAI&#x2F;DeepResearch（o3 模型驱动，准确率超基准）、Perplexity&#x2F;DeepResearch（免费开放，侧重快速响应）
开源生态：nickscamara&#x2F;open-deepresearch、mshumer&#x2F;OpenDeepResearcher、GPT-researcher


生态扩展与多模态融合（2025.3 - 至今）

核心进展：本地化部署（Jina-AI&#x2F;node-DeepResearch）、多模态支持（Gemini&#x2F;DeepResearch）、多智能体协作（Camel-AI&#x2F;OWL）
新参与者：Anthropic Claude&#x2F;Research（2025.4，支持可验证引用）、Manus、AutoGLM-Research



三、系统比较与评估分析3.1 核心系统技术参数对比


系统名称
基础模型
上下文长度
关键能力亮点
典型应用场景



OpenAI&#x2F;DeepResearch
o3
最高 200k tokens
多步推理、学术数据库集成、高准确率报告
学术研究、金融分析


Gemini&#x2F;DeepResearch
Gemini 2.5 Pro
1M tokens
多模态处理、多智能体协作、长文本分析
科学发现、商业智能


Perplexity&#x2F;DeepResearch
DeepSeek-R1
128K tokens
实时源聚合、免费访问、快速响应
个人知识管理、市场趋势


Manus
未明确
未明确
150 + 服务集成、战略决策支持
企业商业分析


AutoGLM-Research
ChatGLM
依赖模型（DOM）
GUI 交互、移动端支持、本地部署
个性化学习、小范围研究


3.2 性能基准表现（定量指标）


系统名称
HLE 得分（%）
MMLU 得分（%）
GAIA（pass@1，%）
HotpotQA 得分（%）



OpenAI&#x2F;DeepResearch
26.6
-
67.36
-


Gemini 2.5 Pro
18.8
-
-
-


Perplexity&#x2F;DeepResearch
21.1
-
-
-


Grok3Beta
-
92.7
-
-


Manus
-
-
86.5
-


Agent-RL&#x2F;ReSearch
-
-
-
37.51


3.3 应用领域适配性
学术研究：OpenAI&#x2F;DeepResearch、Camel-AI&#x2F;OWL 表现突出，支持 ArXiv、PubMed 等数据库集成，能识别研究方法、生成 IEEE&#x2F;APA 格式引用，适配文献综述、假设生成场景。
商业智能：Gemini&#x2F;DeepResearch、Manus 擅长市场分析（SEC 文件、行业报告）、SWOT 框架应用，输出可直接用于战略决策的 executive summary。
教育领域：HKUDS&#x2F;Auto-Deep-Research、OpenManus 提供个性化学习路径，支持多难度解释（适配不同知识水平学习者），辅助课程设计与研究技能培训。
个人知识管理：Perplexity&#x2F;DeepResearch（多设备支持）、nickscamara&#x2F;open-deep-research（本地文件集成），适配信息整理、兴趣驱动学习场景。

四、技术挑战与伦理考量
技术挑战

信息准确性：LLM 幻觉问题，需通过多源验证（如 Perplexity 的源多样性过滤）、矛盾检测（如 Gemini 的不确定性建模）缓解
计算效率：商业系统（如 OpenAI）响应时间 5-30 分钟，开源系统（如 nickscamara&#x2F;open-deepresearch）资源需求高，本地部署需优化
系统集成：工具链兼容性（如 API 版本差异）、跨平台部署（桌面 &#x2F; 移动端适配）


伦理问题

隐私安全：用户查询隔离（商业系统）、本地部署支持（开源系统如 OpenManus），需合规 GDPR、CCPA 等法规
知识产权：引用完整性（如 OpenAI 的语句级引用链接）、版权合规（如 Jina-AI 的许可证分类）
可及性：计算资源门槛（需 GPU 支持）、技术 expertise 要求（开源系统部署复杂），可能加剧数字鸿沟



五、未来研究方向
先进推理架构：神经符号融合（结合逻辑推理与 LLM）、因果推理（干预建模）、外部记忆框架（突破上下文限制）
多模态融合：科学图像分析（图表 &#x2F; 实验图像）、视频 &#x2F; 音频内容处理（学术讲座、实验演示）、跨模态一致性验证
领域专业化：科学领域（物理 &#x2F; 化学专用模型）、法律领域（案例推理、合规分析）、医疗领域（临床证据合成、个性化适配）
人机协作与标准化：交互工作流（动态查询优化）、通用接口协议（如 Anthropic MCP、Google A2A）、联合知识创建（人机协同报告生成）


一些思考问题 1：Deep Research 系统与传统 AI 助手（如 ChatGPT）的核心区别是什么？这些区别如何影响其在学术研究中的应用价值？核心区别体现在三个维度：

工作流能力：Deep Research 系统支持 “文献检索 - 实验设计 - 数据分析 - 报告生成” 的端到端自动化，如 OpenAI&#x2F;DeepResearch 可整合 ArXiv 文献与统计工具；传统 AI 助手仅能响应单次查询，无法自主串联多步研究任务。
工具与环境交互：Deep Research 可集成 16000 + 专用 API（ToolLLM 支持）、动态网页导航（如 Nanobrowser）、多格式文档处理（PDF &#x2F; 表格）；传统 AI 助手缺乏工具调用能力，依赖用户手动输入信息。
推理深度：Deep Research 采用链 &#x2F; 树状推理（如 Gemini 的树状推理）、百万级 token 上下文（Gemini 2.5 Pro 达 1M tokens），能处理长文本与复杂逻辑；传统 AI 助手推理链短，上下文窗口有限（如 ChatGPT 免费版 4k tokens）。

这些区别使 Deep Research 在学术研究中更具价值：可自动化系统综述（处理数千篇文献）、识别研究方法漏洞、生成带可验证引用的报告，大幅降低手动工作量，如 OpenAI&#x2F;DeepResearch 在医学研究中分析数千篇论文以识别干预 efficacy 模式。
问题 2：当前主流 Deep Research 系统（商业与开源）在技术性能与应用适配性上有何差异？企业与学术用户应如何选择？答案：商业与开源系统的差异及选择建议如下：
（1）技术性能差异


维度
商业系统（如 OpenAI&#x2F;DeepResearch）
开源系统（如 dzhng&#x2F;deepresearch）



基础模型
专用模型（o3、Gemini 2.5 Pro），性能领先
通用模型（ChatGLM、DeepSeek-R1），需优化


响应时间
5-30 分钟（复杂任务）
更长（依赖本地硬件，需 10 + 分钟）


准确率（HLE 基准）
26.6%（OpenAI）、21.1%（Perplexity）
未公开，实测低于商业系统


维护与更新
自动迭代（如 OpenAI o3→o4-mini）
依赖社区贡献，更新频率低


（2）应用适配性差异
商业系统：适配大规模学术研究（多数据库集成）、企业商业分析（实时市场数据），适合对准确率、稳定性要求高的场景，如药企的临床文献综述、跨国企业的竞品分析。
开源系统：支持本地化部署（数据隐私敏感场景）、定制化开发（如添加领域专用工具），适合预算有限的学术团队（小范围课题研究）、企业内部敏感数据处理（如内部报告生成）。

（3）选择建议
学术用户：若需处理大规模文献（如系统综述）、依赖高准确率（如假设验证），选择商业系统（如 OpenAI&#x2F;DeepResearch）；若数据敏感（如未发表实验数据）、需定制工具链，选择开源系统（如 HKUDS&#x2F;Auto-Deep-Research）。
企业用户：若需实时市场分析（如 Perplexity 的快速响应）、多部门协作（如 Gemini 的多智能体功能），选择商业系统；若需本地化部署（合规要求）、低成本扩展，选择开源系统（如 Jina-AI&#x2F;node-DeepResearch）。

问题 3：当前 Deep Research 系统在解决 “信息准确性” 与 “幻觉” 问题上有哪些关键技术手段？这些手段的局限性是什么？未来可如何改进？答案：当前关键技术手段、局限性及改进方向如下：
（1）当前技术手段
多源验证：商业系统（如 OpenAI&#x2F;DeepResearch）要求关键结论需 2 + 独立源支持，Perplexity&#x2F;DeepResearch 通过 “源多样性过滤”（跨平台聚合信息）降低单一源偏差；开源系统（如 grapeot&#x2F;deep_research_agent）采用源可信度评分（基于发布机构、引用量）。
引用与溯源：OpenAI&#x2F;DeepResearch 实现 “语句级引用链接”，每个结论可跳转至原始文献 &#x2F; 数据；mshumer&#x2F;OpenDeepResearcher 生成结构化报告，明确标注引用来源与 DOI。
不确定性建模：Gemini&#x2F;DeepResearch 区分 “确认信息” 与 “推测内容”，用置信度指标（如高 &#x2F; 中 &#x2F; 低）标注结论可靠性；Agent-RL&#x2F;ReSearch 通过 “矛盾检测算法” 识别冲突信息并提示用户。

（2）局限性
多源验证：依赖源的可用性（如小众领域文献少，无法多源交叉），且无法识别 “同源信息重复”（如不同平台转载同一研究）。
引用机制：仅覆盖公开源（如 ArXiv、PubMed），企业内部报告、未发表数据等私有信息无法溯源；引用格式易出错（如开源系统的 BibTeX 导出可能遗漏字段）。
不确定性建模：置信度指标缺乏统一标准（商业系统各自定义），用户难以判断 “低置信度” 是否需进一步验证；无法处理 “证据不足” 场景（如新兴研究领域无足够数据支持结论）。

（3）未来改进方向
证据质量评估细化：引入领域专用指标，如学术研究中结合 “研究方法严谨性”（样本量、随机对照设计）、商业分析中结合 “数据时效性”（市场数据发布时间）评估源质量。
动态溯源框架：支持私有源（如企业数据库、实验室内部系统）的权限管理与溯源，通过区块链技术确保引用不可篡改。
人机协同验证：设计交互接口，如 HKUDS&#x2F;Auto-Deep-Research 的 “矛盾提示 - 用户决策” 流程，让用户参与高不确定性结论的验证，同时记录反馈以优化模型判断。

]]></content>
      <categories>
        <category>果冻的航海日志</category>
      </categories>
      <tags>
        <tag>LLM</tag>
      </tags>
  </entry>
  <entry>
    <title>Docker</title>
    <url>/gdBlog/2025/09/29/Docker/</url>
    <content><![CDATA[远程镜像仓库本地镜像(镜像是只读的)本地仓库
镜像版本tag版本是完全独立的,last是最新的docker pull&#x2F;push 镜像名docker images 查看本地镜像docker search
容器真正的示例,隔离网络,文件.不同容器可能会争抢资源1容器打包成镜像,再上传到仓库docker psdocker ps -adocekr start&#x2F;stop&#x2F;rm IDdocker commit -a “作者名称” -m “log信息” ID 打包成镜像docker cp 文件目录 容器ID:目标目录   从前拷贝到后docker exec -it ID &#x2F;bin&#x2F;bash  进入容器内部
dickerfile脚本 是可以通过下载文件的一些来创建脚本
网络中的映射一般是端口映射,容器端口8001映射到宿主机的8080
]]></content>
      <categories>
        <category>果冻的航海日志</category>
      </categories>
      <tags>
        <tag>tools</tag>
      </tags>
  </entry>
  <entry>
    <title>Github自动同步脚本</title>
    <url>/gdBlog/2025/10/01/Github%E8%87%AA%E5%8A%A8%E5%90%8C%E6%AD%A5%E8%84%9A%E6%9C%AC/</url>
    <content><![CDATA[实施步骤进入要同步的库中，切换至 Actions，点击后面的 New workflow 项
打开新页面后，点击篮字的 set up a workflow yourself
设置文件名 sync.yml「可自定义，不与其它脚本同名即可」
将下面的脚本填到输入框中，点击右上方 Commit changes 即可：
name: Upstream Syncpermissions:  contents: writeon:  schedule:    - cron: &quot;0 0 * * *&quot; # every day  workflow_dispatch:jobs:  sync_latest_from_upstream:    name: Sync latest commits from upstream repo    runs-on: ubuntu-latest    if: $&#123;&#123; github.event.repository.fork &#125;&#125;    steps:      # Step 1: run a standard checkout action      - name: Checkout target repo        uses: actions/checkout@v4      # Step 2: run the sync action      - name: Sync upstream changes        id: sync        uses: aormsby/Fork-Sync-With-Upstream-action@v3.4        with:          upstream_sync_repo: arnidan/nsfw-api          upstream_sync_branch: main          target_sync_branch: main          target_repo_token: $&#123;&#123; secrets.GITHUB_TOKEN &#125;&#125; # automatically generated, no need to set          # Set test_mode true to run tests instead of the true action!!          test_mode: false      - name: Sync check        if: failure()        run: |          echo &quot;[Error] 由于上游仓库的 workflow 文件变更，导致 GitHub 自动暂停了本次自动更新，您需要手动 Sync Fork 一次。&quot;          echo &quot;[Error] Due to a change in the workflow file of the upstream repository, GitHub has automatically suspended the scheduled automatic update. You need to manually sync your fork.&quot;          exit 1]]></content>
      <categories>
        <category>果冻的奇妙小工具</category>
      </categories>
      <tags>
        <tag>tools</tag>
      </tags>
  </entry>
  <entry>
    <title>Hexo通过插件隐藏指定文章</title>
    <url>/gdBlog/2025/10/02/Hexo%E9%80%9A%E8%BF%87%E6%8F%92%E4%BB%B6%E9%9A%90%E8%97%8F%E6%8C%87%E5%AE%9A%E6%96%87%E7%AB%A0/</url>
    <content><![CDATA[由于本人感觉记不住昨天或者前几天发生的事情, 因此打算写点日记, 但是又没必要让所有人看到, 不必在首页展示, 因此想添加文章隐藏功能. 同时又不想修改Butterfly主题源代码, 原因在于作为一名开发者，还是想使用增量更新，而不是修改原来的内容. 终于！找到了hexo-hide-posts这个Hexo 插件，它能够隐藏指定的文章，并使它们仅可通过链接访问
可以直接参考README_ZH | hexo-hide-posts来进行配置，这里记录我的配置步骤。
安装在项目目录执行以下命令安装插件：
npm install hexo-hide-posts
安装完成后在项目根目录的_config.yml中添加如下内容：
# 文章隐藏：https://github.com/prinsss/hexo-hide-postshide_posts:  enable: true # 是否启用 hexo-hide-posts  filter: hidden # 隐藏文章的 front-matter 标识，也可以改成其他你喜欢的名字  noindex: true # 为隐藏的文章添加 noindex meta 标签，阻止搜索引擎收录  # 设置白名单，白名单中的 generator 可以访问隐藏文章  # 常见的 generators 有：index, tag, category, archive, sitemap, feed, etc.  # allowlist_generators: []  allowlist_generators: [&#x27;*&#x27;]    # 设置黑名单，黑名单中的 generator 不可以访问隐藏文章  # 如果同时设置了黑名单和白名单，白名单的优先级更高  # blocklist_generators: [&#x27;*&#x27;]  blocklist_generators: [&#x27;index&#x27;, &#x27;feed&#x27;]
使用如果在_config.yml中的配置为filter: hidden，则在文章的 front-matter 中添加 hidden: true 即可隐藏文章，如：
---  title: &#x27;被隐藏的文章&#x27;  date: &#x27;2025-10-02&#x27;  hidden: true  ---

特别鸣谢文章作者: InsectMk原文链接:https://insectmk.cn/posts/9c83ed78/参考文章:README_ZH | hexo-hide-posts
]]></content>
      <categories>
        <category>果冻的奇妙小工具</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title>MCP(Model Context Protocol模型上下文协议)</title>
    <url>/gdBlog/2025/09/30/MCP/</url>
    <content><![CDATA[MCP 起源于 2024 年 11 月 25 日, 定义了应用程序和 AI 模型之间交换上下文信息的方式。这使得开发者能够以一致的方式将各种数据源、工具和功能连接到 AI 模型（一个中间协议层），就像 USB-C 让不同设备能够通过相同的接口连接一样。MCP 的目标是创建一个通用标准，使 AI 应用程序的开发和集成变得更加简单和统一。
起源思考我认为 MCP 的出现是 prompt engineering 发展的产物。更结构化的上下文信息对模型的 performance 提升是显著的。在构造 prompt 时，希望能提供一些更 specific 的信息（比如本地文件，数据库，一些网络实时信息等）给模型，这样模型更容易理解真实场景中的问题。
第一阶段手工输入. 想象一下没有 MCP 之前我们会怎么做？我们可能会人工从数据库中筛选或者使用工具检索可能需要的信息，手动的粘贴到 prompt 中。随着我们要解决的问题越来越复杂，手工把信息引入到 prompt 中会变得越来越困难。
第二阶段 function call. 为了克服手工 prompt 的局限性，许多 LLM 平台（如 OpenAI、Google）引入了 function call 功能。这一机制允许模型在需要时调用预定义的函数来获取数据或执行操作。但是 function call 依赖平台, 不同 LLM 平台的 function call API 实现差异较大。例如，OpenAI 的函数调用方式与 Google 的不兼容，开发者在切换模型时需要重写代码.
第三阶段 MCP. 数据与工具本身是客观存在的，只不过我们希望将数据连接到模型的这个环节可以更智能更统一。Anthropic 基于这样的痛点设计了 MCP，充当 AI 模型的”万能转接头”，让 LLM 能轻松的获取数据或者调用工具.
原理先说过程:用户在 Claude 客户端（如 Claude Desktop、Cursor）输入问题后，客户端会先将问题发送给 Claude 大模型；Claude 接收问题后，会分析当前可用的工具（如文档解析、数据查询等）并确定需调用的工具类型及数量；接着客户端通过 MCP Server（模型上下文协议服务端），以标准化方式执行 Claude 选定的工具；工具执行完成后，其处理结果会通过 MCP 回传给 Claude；Claude 再结合用户原始问题与工具执行结果，构建最终的 Prompt 并生成自然语言回应；最后，客户端将这份回应展示给用户，完成整个交互流程。
关于模型如何选择工具模型是通过 prompt 来确定当前有哪些工具。我们通过将工具的具体使用描述以文本的形式传递给模型，供模型了解有哪些工具以及结合实时情况进行选择。参考代码中的注释：
... # 省略了无关的代码async def start(self):    # 初始化所有的 mcp server    for server in self.servers:        await server.initialize()​    # 获取所有的 tools 命名为 all_tools    all_tools = []    for server in self.servers:        tools = await server.list_tools()        all_tools.extend(tools)​    # 将所有的 tools 的功能描述格式化成字符串供 LLM 使用    # tool.format_for_llm() 我放到了这段代码最后，方便阅读。    tools_description = &quot;\n&quot;.join(        [tool.format_for_llm() for tool in all_tools]    )​    # 这里就不简化了，以供参考，实际上就是基于 prompt 和当前所有工具的信息    # 询问 LLM（Claude） 应该使用哪些工具。    system_message = (        &quot;You are a helpful assistant with access to these tools:\n\n&quot;        f&quot;&#123;tools_description&#125;\n&quot;        &quot;Choose the appropriate tool based on the user&#x27;s question. &quot;        &quot;If no tool is needed, reply directly.\n\n&quot;        &quot;IMPORTANT: When you need to use a tool, you must ONLY respond with &quot;        &quot;the exact JSON object format below, nothing else:\n&quot;        &quot;&#123;\n&quot;        &#x27;    &quot;tool&quot;: &quot;tool-name&quot;,\n&#x27;        &#x27;    &quot;arguments&quot;: &#123;\n&#x27;        &#x27;        &quot;argument-name&quot;: &quot;value&quot;\n&#x27;        &quot;    &#125;\n&quot;        &quot;&#125;\n\n&quot;        &quot;After receiving a tool&#x27;s response:\n&quot;        &quot;1. Transform the raw data into a natural, conversational response\n&quot;        &quot;2. Keep responses concise but informative\n&quot;        &quot;3. Focus on the most relevant information\n&quot;        &quot;4. Use appropriate context from the user&#x27;s question\n&quot;        &quot;5. Avoid simply repeating the raw data\n\n&quot;        &quot;Please use only the tools that are explicitly defined above.&quot;    )    messages = [&#123;&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: system_message&#125;]​    while True:        # Final... 假设这里已经处理了用户消息输入.        messages.append(&#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: user_input&#125;)​        # 将 system_message 和用户消息输入一起发送给 LLM        llm_response = self.llm_client.get_response(messages)​    ... # 后面和确定使用哪些工具无关    ​class Tool:    &quot;&quot;&quot;Represents a tool with its properties and formatting.&quot;&quot;&quot;​    def __init__(        self, name: str, description: str, input_schema: dict[str, Any]    ) -&gt; None:        self.name: str = name        self.description: str = description        self.input_schema: dict[str, Any] = input_schema​    # 把工具的名字 / 工具的用途（description）和工具所需要的参数（args_desc）转化为文本    def format_for_llm(self) -&gt; str:        &quot;&quot;&quot;Format tool information for LLM.​        Returns:            A formatted string describing the tool.        &quot;&quot;&quot;        args_desc = []        if &quot;properties&quot; in self.input_schema:            for param_name, param_info in self.input_schema[&quot;properties&quot;].items():                arg_desc = (                    f&quot;- &#123;param_name&#125;: &#123;param_info.get(&#x27;description&#x27;, &#x27;No description&#x27;)&#125;&quot;                )                if param_name in self.input_schema.get(&quot;required&quot;, []):                    arg_desc += &quot; (required)&quot;                args_desc.append(arg_desc)​        return f&quot;&quot;&quot;Tool: &#123;self.name&#125;Description: &#123;self.description&#125;Arguments:&#123;chr(10).join(args_desc)&#125;&quot;&quot;&quot;
因此 描述工具的文本input_schema是我们要写的.  大部分情况下，当使用装饰器 @mcp.tool() 来装饰函数时，对应的 name 和 description 等其实直接源自用户定义函数的函数名以及函数的 docstring 等。
@classmethoddef from_function(    cls,    fn: Callable,    name: str | None = None,    description: str | None = None,    context_kwarg: str | None = None,) -&gt; &quot;Tool&quot;:    &quot;&quot;&quot;Create a Tool from a function.&quot;&quot;&quot;    func_name = name or fn.__name__ # 获取函数名​    if func_name == &quot;&lt;lambda&gt;&quot;:        raise ValueError(&quot;You must provide a name for lambda functions&quot;)​    func_doc = description or fn.__doc__ or &quot;&quot; # 获取函数 docstring    is_async = inspect.iscoroutinefunction(fn)        ... # 更多请参考原始代码...
模型是通过 prompt engineering，即提供所有工具的结构化描述和 few-shot 的 example 来确定该使用哪些工具。
工具执行与结果反馈机制其实工具的执行就比较简单和直接了。承接上一步，我们把 system prompt（指令与工具调用描述）和用户消息一起发送给模型，然后接收模型的回复。当模型分析用户请求后，它会决定是否需要调用工具：

无需工具时：模型直接生成自然语言回复。
需要工具时：模型输出结构化 JSON 格式的工具调用请求。

如果回复中包含结构化 JSON 格式的工具调用请求，则客户端会根据这个 json 代码执行对应的工具。具体的实现逻辑都在 process_llm_response 中，代码，逻辑非常简单。
如果模型执行了 tool call，则工具执行的结果 result 会和 system prompt 和用户消息一起重新发送给模型，请求模型生成最终回复。
如果 tool call 的 json 代码存在问题或者模型产生了幻觉怎么办呢？通过阅读代码 发现，我们会 skip 掉无效的调用请求。
]]></content>
      <categories>
        <category>果冻的航海日志</category>
      </categories>
      <tags>
        <tag>LLM</tag>
        <tag>tools</tag>
      </tags>
  </entry>
  <entry>
    <title>JSON</title>
    <url>/gdBlog/2025/09/29/JSON/</url>
    <content><![CDATA[JSON是一种文本数据格式!
json和xml之间一般可以转换json文本实际上是一条字符串json的四种类型不允许嵌套,字符串,数值,布尔,null(你也没法嵌套)两种可以嵌套:哈希值:「“字符串”:剩下六种随便,“”:“”」最后一个键值对后不能加“,”数组:[],用“,”隔开,最后一个不加,六种随便一种,不用统一类型
空格和换行无所谓
一般数据量不大,大了的话一般用xml,比较方便.
]]></content>
      <categories>
        <category>果冻的航海日志</category>
      </categories>
      <tags>
        <tag>tools</tag>
      </tags>
  </entry>
  <entry>
    <title>MVC（Model-View-Controller，模型 - 视图 - 控制器）</title>
    <url>/gdBlog/2025/09/30/MVC/</url>
    <content><![CDATA[简介MVC（Model-View-Controller，模型 - 视图 - 控制器）是一种软件架构设计模式，其核心思想是分离关注点，通过将应用程序划分为三个相互关联但功能独立的组件.
各组件的具体作用模型（Model）模型是应用程序的数据中心和业务逻辑核心，负责管理应用程序的数据和处理业务规则，是整个应用的 “大脑”。
主要职责：

存储和管理应用程序的数据（可以是数据库、内存数据结构等）
实现核心业务逻辑和数据处理规则
提供数据访问接口（供控制器调用）
当数据发生变化时，通知相关视图进行更新（观察者模式）
独立于视图和控制器，不关心数据如何展示和用户如何操作

视图（View）视图是应用程序的用户界面，负责数据的展示和用户交互的呈现，是用户能直接看到和操作的部分
主要职责：

从模型获取数据并以特定形式展示给用户（如网页、桌面窗口、移动端界面）
接收用户的界面操作（如点击按钮、输入文本等），但不处理这些操作的业务逻辑
不直接与模型交互，通过控制器获取需要展示的数据
一个模型可以对应多个视图（如同一组数据可以用表格、图表、列表等不同形式展示）

控制器（Controller）控制器是模型和视图之间的协调者，负责处理用户输入并协调模型和视图完成相应操作。
主要职责：

接收并解析用户的输入请求（来自视图）
根据用户请求调用相应的模型方法处理数据
决定处理完成后使用哪个视图展示结果
不处理业务逻辑，也不负责数据展示，仅负责流程控制
维护模型和视图之间的映射关系

MVC 工作流程
用户交互：用户通过视图（如点击按钮、提交表单）发起操作请求
请求传递：视图将用户请求传递给对应的控制器
业务处理：控制器调用相应的模型方法，处理业务逻辑和数据
数据更新：模型处理完成后更新数据状态，必要时通知视图
视图渲染：控制器选择合适的视图，视图从模型获取最新数据并展示给用户

]]></content>
      <categories>
        <category>果冻的航海日志</category>
      </categories>
  </entry>
  <entry>
    <title>RAG（Retrieval-AugmentedGeneration检索增强生成</title>
    <url>/gdBlog/2025/10/01/RAG/</url>
    <content><![CDATA[写在前面RAG 是一种新兴的 AI 技术，它结合了信息检索和生成式 AI 的优势，能够在处理复杂任务时提供更准确和相关的答案。RAG 的实现过程相对简单，但是它的核心在于如何检索到相关的信息，以及如何将这些信息与 LLM 结合，这就是 RAG 的关键所在。
LLM的局限性RAG（Retrieval-Augmented Generation 检索增强生成）正如其字面意思，是通过检索信息来增强 LLM生成的能力。RAG 是一种新兴的 AI 技术，它结合了信息检索和生成式 AI 的优势，能够在处理复杂任务时提供更准确和相关的答案。
RAG 的工作原理是：首先通过检索引擎从一个大型知识库中获取相关信息，然后将这些信息通过 Prompt 工程与 LLM 结合，生成最终的答案。
为什么需要这样做呢？ LLM的特点：

会回答你很多不存在的东西，编造一些不存在的事实，或者是对事实进行错误的推理，LLM 回答不存在的东西的现象被称为幻觉（Hallucination）。
尤其不擅长于最新的技术和最近的热点事件（注：这里排除掉能够使用联网能力的产品），因为 LLM 训练时使用的数据集是有截止日期的。

RAG实现流程是: 用户输入问题-检索相关信息-将信息与LLM结合-生成最后答案
实现就是这么简单，但是它的核心在于如何检索到相关的信息，以及如何将这些信息与 LLM 结合，这就是 RAG 的关键所在. 对于一个RAG 产品来说，检索的质量和 LLM 的生成能力是两个最重要的指标，它们直接决定了给用户呈现的最终效果，而对 AI 落地产品来说，检索的质量更是重中之重，因为 AI 落地产品的核心就是要解决用户的问题，而不是让用户来解决 AI 的问题。
那么，如何提高检索的质量呢？
模糊搜索&#x2F;关键词匹配？那肯定不行，用户输入的问题可能是抽象的，常规的模糊搜索算法可能在用户一长段问题中找不到任何相关的内容，或者是找到的内容和用户的问题完全不相关，自然语言暗藏了太多的语义信息，模糊搜索和关键词匹配都无法捕捉到这些信息。
大语言模型的核心就是语言模型，语言模型的核心就是概率分布，RAG 的检索过程其实就是一个概率分布的过程，但是 LLM 是需要经过训练的，训练的过程就是不断的调整模型参数，使得模型能够更好的拟合数据集中的概率分布
自然语言检索核心: EmbeddingEmbedding模型就是用来解决这类自然语言检索问题的，它的核心思想就是将自然语言转换成向量（Vector），然后通过计算向量之间的距离来判断它们之间的相似度。
实例假设我们有一个问题：“你能给我推荐一个用一个红色水果做品牌名称的科技公司出的产品吗？”
假设我们有一个 Embedding 模型，它能够将这些自然语言转换成抽象高维度向量，向量的每一个维度都代表了一个特征，比如颜色、形状、大小等等。
假设我们有一个知识库，里面有很多关于水果的品牌名称，比如“手机”、“电动汽车”、“智能台灯”，“苹果”、“香蕉”、“西瓜”等等。
假设我们有一个检索算法，它能够计算向量之间的距离，比如欧几里得距离、余弦相似度等。
我们使用余弦相似度来计算向量之间的距离，那么我们可以将问题转换成向量，比如“你能给我推荐一个用一个红色水果做品牌名称的科技公司出的产品吗？”可以转换成一个向量 A，而知识库中的“苹果”，“手机”，“电动汽车”，“智能台灯”等等也可以转换成向量 B，C，D 等等。
对于文本类问题，我们通常使用余弦相似度来计算向量之间的距离，余弦相似度的公式如下：
![[ObsidianPicture&#x2F;Pasted image 20251001091142.png]]
通过此公式，我们可以计算出向量 A 和 B 等之间的余弦相似度，余弦相似度的值在-1~1之间，值越大表示两个向量越相似。 如果余弦相似度的值大于某个阈值，比如 0.8 ，那么我们就可以认为这两个向量是相似的，也就是问题和知识库中的内容是相关的。
这个时候，我们将检索内容通过 Prompt 工程再喂给 LLM，LLM 就会根据检索到的内容生成最终的答案。
在实际开发中，Embedding 模型(这个也有不同的模型, 效果和性能上有差别)可能是超高维度的，通常在 512 维以上，因此对于传统关系型数据库来说，存储和检索的成本都非常高，因此我们通常会使用一些专门的向量数据库来存储和检索向量，比如 Milvus、Pinecone、Weaviate 等等。
这些向量数据库通常会使用一些高效的索引算法来加速向量的检索，比如 HNSW、IVF、PQ 等等。向量数据库通常已处理好了这些算法，你只需要调用相关 API 即可。
Embedding前的分块在预处理阶段，在Embdding前还有一个文档解析与分块的过程，因为在通常情况下，你所使用的Embedding 模型能够处理的文本长度是有限制的，而构建知识库的过程就是将文档解析成小块，然后将这些小块进行向量化，存入向量数据库中.
在分块时，你可以使用不同的算法，你可以固定大小分块，比如说每 512 字符分成一块，但是这样可能会导致语义不连贯，或者是分块过小，导致向量数据库存储的向量数量过多，成本过高。
基于内容意图分块，可以根据内容的语义和上下文进行分块，这样可以更好的保留语义信息，提高检索的质量。
RAG 分类上述提到的是 Naive RAG
还有其他的RAG:

Advanced RAG：它在 Naive RAG 的基础上，增加了一些优化，在检索前、检索中、检索后都增加了一些优化，比如使用多种检索算法进行组合，使用多种 LLM 进行组合，使用多种 Prompt 模板进行组合等等，这样可以提高检索的质量和生成的效果。

Modular RAG：它将 RAG 的各个部分进行模块化，您可以根据自己的需求选择合适的模块进行组合，比如使用不同的检索算法、不同的 LLM、不同的 Prompt 模板等等，这样可以提高 RAG 的灵活性和可扩展性。


使用传统的 Naive RAG 可能会导致检索的质量和生成的效果不理想，也有可能使用 Rerank（重排序）算法来对检索的结果进行重排序，或者使用 Chain of Thought（思维链）来对生成的结果进行优化，这些都是 RAG 的变种实现方式。
]]></content>
      <categories>
        <category>果冻的航海日志</category>
      </categories>
      <tags>
        <tag>LLM</tag>
        <tag>tools</tag>
      </tags>
  </entry>
  <entry>
    <title>元芳接入Gemini</title>
    <url>/gdBlog/2025/10/02/%E5%85%83%E8%8A%B3%E6%8E%A5%E5%85%A5Gemini/</url>
    <content><![CDATA[写在前面 我的实现方式gemini是google的 所以需要注册google对应的账号,由于需要翻墙再加上需要资金往来,所以放弃这条路了.最后选择使用国内的代理网站 简易,调用了一个 API 接口, 确实挺好用的.注册链接https://jeniya.top/register?aff=tUUD
了解Gemini历程2023 年 4 月，谷歌母公司 Alphabet 首席执行官桑达尔・皮查伊合并了两个大型人工智能团队，开启 OpenAI 计划。2023 年 12 月 6 日，谷歌正式推出 Gemini 1.0 版本，包括 Gemini Ultra、Gemini Pro 和 Gemini Nano 三个不同规格。2024 年 2 月 15 日，谷歌发布 Gemini 1.5，后续又不断对其进行升级和优化，如 2024 年 5 月 15 日更新升级 Gemini 1.5 Pro 版本，同时推出 Gemini 1.5 Flash 轻量化小模型。2025 年 3 月 26 日，谷歌正式推出新一代人工智能推理模型 Gemini 2.5。
模型规格及特点Gemini Ultra：是 Gemini 中能力最强的模型，适用于处理高度复杂的任务，如在各种推理和多模态任务中表现出色。它在 MMLU 基准测试中的得分率高达 90.0%，首次超越了人类专家。
 Gemini Pro：适用于多任务，在成本和延迟方面进行了性能优化，具有推理功能和广泛的多模态能力，可在广泛的任务范围内提供显著的性能。
Gemini Nano：是最高效的模型，用于特定任务和移动设备。该模型训练了两个版本的 Nano，参数分别为 1.8B（Nano-1）和 3.25B（Nano-2），分别针对低内存和高内存器件，采用 4 位量化部署。
本次接入Gemini 2.5 系列模型🔥 gemini-2.5-flash-lite
速度: 最快响应时间
成本: 最经济实惠
能力: 基础文本理解和生成
最佳用途: 聊天机器人、简单问答、内容摘要

⚡ gemini-2.5-flash
速度: 快速响应
成本: 平衡性价比
能力: 文本+图像理解，多模态基础任务
最佳用途: 通用AI应用、客户服务、内容创作

🎯 gemini-2.5-pro
速度: 响应较慢但质量最高
成本: 最高定价
能力: 复杂推理、专业分析、多模态高级任务
最佳用途: 研究分析、代码生成、复杂问题解决

🖼️ gemini-2.5-flash-image-preview
速度: 针对图像优化
成本: 中等定价
能力: 专业图像识别、视觉分析
最佳用途: 图像描述、OCR、视觉内容分析

选择建议
预算敏感: 选择 flash-lite
平衡需求: 选择 flash
高质量要求: 选择 pro
图像处理: 选择 flash-image-preview

输出接口&#123;  &quot;system_instruction&quot;: &#123;    &quot;parts&quot;: [      &#123;        &quot;text&quot;: &quot;You are an asistant.&quot;      &#125;    ]  &#125;,  &quot;contents&quot;: [    &#123;      &quot;role&quot;: &quot;user&quot;,      &quot;parts&quot;: [        &#123;          &quot;text&quot;: &quot;$&#123;content&#125;&quot;        &#125;      ]    &#125;  ]&#125;

官方接口官方文档：https://ai.google.dev/gemini-api/docs/text-generation?hl=zh-cn#multi-turn-conversations
]]></content>
      <categories>
        <category>果冻的航海日志</category>
      </categories>
      <tags>
        <tag>LLM</tag>
        <tag>元芳</tag>
      </tags>
  </entry>
  <entry>
    <title>元芳接入DeepSearch</title>
    <url>/gdBlog/2025/10/02/%E5%85%83%E8%8A%B3%E6%8E%A5%E5%85%A5DeepSearch/</url>
    <content><![CDATA[实现过程部署在虎符上, 因此Flask Web服务, 最后提供HTTP API接口.在元芳上新增工具为[MCP]问答_DeepResearch深度思考_服务,[MCP]问答_arXiv_服务(并没有用到在应用上), 新增应用为Deep Research应用.

首先使用了DashScope翻译服务完成中译英, 因为arXiv的中文搜索不太行
使用官方提供的arXiv库实现arXiv论文检索
最后使用DeepSeek推理模型, 将论文检索的内容作为输入再加上用户原本的输入作为最终的输入给DeepSeekR1模型(在元芳上)
返回1.论文 2.深度思考 3.最后回答 作为输出.相当于实现了[[DeepSearch]]

备份在github上了Deepsearch
流程核心功能1. 多语言智能处理
中文检测与翻译 ：自动检测用户查询是否为中文，如果是则调用DashScope API翻译为英文
翻译容错机制 ：翻译失败时回退到原始查询，确保系统稳定性

2. 学术资源搜索
arXiv论文检索 ：将翻译后的英文查询用于搜索相关学术论文
结果格式化 ：提取论文标题和PDF链接，生成易读的参考信息

3. AI深度推理
DeepSeek Reasoner模型 ：使用专门的推理模型进行深度思考
上下文增强 ：结合用户查询和论文信息生成全面回答
结构化响应 ：提取角色、内容和推理过程三个维度的信息

4. 完整响应返回
双数据源 ：同时返回论文搜索结果和AI生成的回答
错误处理 ：完善的异常捕获机制，确保服务稳定性

技术亮点
多API集成 ：DashScope翻译 + arXiv搜索 + DeepSeek推理
智能语言处理 ：中英文无缝切换
学术级回答 ：基于最新研究论文提供权威解答这个DeepSearch系统特别适合需要学术背景知识的深度问答场景，能够为用户提供基于最新研究的专业回答。

]]></content>
      <categories>
        <category>果冻的航海日志</category>
      </categories>
      <tags>
        <tag>元芳</tag>
      </tags>
  </entry>
  <entry>
    <title>外链播放器</title>
    <url>/gdBlog/2023/05/05/%E5%A4%96%E9%93%BE%E6%92%AD%E6%94%BE%E5%99%A8/</url>
    <content><![CDATA[为Hexo添加外链播放器








&lt;div style=&quot;position: relative; padding: 30% 45%;&quot;&gt;&lt;iframe style=&quot;position: absolute; width: 100%; height: 100%; left: 0; top: 0;&quot; src=&quot;https://player.bilibili.com/player.html?aid=76053337&amp;;bvid=BV11J41127DF&amp;cid=130096191&amp;page=1&amp;as_wide=1&amp;high_quality=1&amp;danmaku=0&quot; frameborder=&quot;no&quot; scrolling=&quot;no&quot;&gt;&lt;/iframe&gt;&lt;/div&gt;
稍微解释一下上面代码的含义：
page -&gt; 起始下标为 1 (默认值也是为1)
as_wide -&gt; 是否宽屏 【1: 宽屏, 0: 小屏】
high_quality -&gt; 是否高清 【1: 高清(最高1080p) &#x2F; 0: 最低视频质量(默认)】
danmaku -&gt; 是否开启弹幕 【1: 开启(默认), 0: 关闭】
allowfullscreen -&gt; allowfullscreen&#x3D; “ture” 允许全屏，使用该参数可以在浏览器中全屏播放 作者：Mackxin https://www.bilibili.com/read/cv6775208/ 出处：bilibili

&lt;iframe src=&quot;//player.bilibili.com/player.html?bvid=BV1PC4y1t77X&amp;page=1&amp;danmaku=0&amp;high_quality=1&quot; scrolling=&quot;no&quot; border=&quot;0&quot; frameborder=&quot;no&quot; framespacing=&quot;0&quot; allowfullscreen=&quot;true&quot;&gt;&lt;/iframe&gt;

&lt;div style=&quot;position: relative; width: 100%; height: 0; padding-bottom: 75%;&quot;&gt;    &lt;iframe src=&quot;//player.bilibili.com/player.html?bvid=BV1PC4y1t77X&amp;page=1&amp;danmaku=0&amp;high_quality=1&quot; scrolling=&quot;no&quot; border=&quot;0&quot; frameborder=&quot;no&quot; framespacing=&quot;0&quot; allowfullscreen=&quot;true&quot; style=&quot;position: absolute; width: 100%; height: 100%; left: 0; top: 0;&quot;&gt;&lt;/iframe&gt;&lt;/div&gt;

&lt;div style=&quot;position: relative; width: 100%; height: 0; padding-bottom: 75%;&quot;&gt;    &lt;iframe src=&quot;//player.bilibili.com/player.html?bvid=BV1PC4y1t77X&amp;page=1&amp;danmaku=0&amp;high_quality=1&quot; scrolling=&quot;no&quot; border=&quot;0&quot; frameborder=&quot;no&quot; framespacing=&quot;0&quot; allowfullscreen=&quot;true&quot; style=&quot;position: absolute; width: 100%; height: 100%; left: 0; top: 0;&quot; sandbox=&quot;allow-top-navigation allow-same-origin allow-forms allow-scripts&quot;&gt;&lt;/iframe&gt;&lt;/div&gt;]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title>正则表达式</title>
    <url>/gdBlog/2025/09/29/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/</url>
    <content><![CDATA[正则表达式（Regular Expression，简称 regex 或 regexp）是一种用于匹配、查找和处理文本的强大模式描述语言。它通过一系列预定义的字符和规则，构建出能够精确匹配特定文本模式的表达式，广泛应用于文本搜索、验证、替换、提取等场景（如表单验证、日志分析、代码解析等）。
正则表达式的入门门槛稍高，但掌握后能极大提升文本处理效率。实际使用时，可借助在线工具（如 Regex101）实时测试和调试表达式。
高级版关键字搜索性能一般,但无所谓不差这点,功能强大,易于维护
字符集合g[ a o]t&#x3D;get or got[ a-z]就是a到z ; [^b-c ]除了b到c
* 出现0或更多 {0}+  1 and more. {1}?  0 or 1            {0,1}{3} 3{3,}. 3 and more{3.6}  3 to 6
^代表开始  $代表结束
]]></content>
      <categories>
        <category>果冻的航海日志</category>
      </categories>
      <tags>
        <tag>tools</tag>
      </tags>
  </entry>
  <entry>
    <title>元芳接入MCP</title>
    <url>/gdBlog/2025/10/02/%E5%85%83%E8%8A%B3%E6%8E%A5%E5%85%A5MCP/</url>
    <content><![CDATA[项目特点(写在前面)基于 Flask + MySQL + ChromaDB 的 工具管理系统 ,部署在虎符上,为元芳mcp-client工具提供服务，主要用于管理和搜索各种元芳工具，并集成MCP,向量检索,内含元芳执行器不依赖其他元芳版本只依赖数据库。
返回接口执行失败
&#123;    &quot;log&quot;: [        &#123;            &quot;MCP_select_tool_name&quot;: &quot;工具名称&quot;,            &quot;yf_tool_parameters&quot;: &quot;参数&quot;        &#125;    ],    &quot;error&quot;: &quot;错误信息或最终结果&quot;&#125;
执行成功
&#123;    &quot;log&quot;: [        &#123;&quot;name&quot;: &quot;工具1&quot;, &quot;description&quot;: &quot;描述1&quot;&#125;,        &#123;&quot;name&quot;: &quot;工具2&quot;, &quot;description&quot;: &quot;描述2&quot;&#125;,        &#123;&quot;name&quot;: &quot;工具3&quot;, &quot;description&quot;: &quot;描述3&quot;&#125;,        &#123;	        &quot;MCP_select_tool_name&quot;: &quot;实际选择的工具名&quot;, 	        &quot;yf_tool_parameters&quot;: &quot;具体参数&quot;	    &#125;    ],    &quot;response&quot;: &quot;工具执行的具体结果内容&quot;&#125;

类Flask应用├── CustomEmbeddingFunction (向量嵌入)├── Tool (数据模型)├── ToolCapability (工具管理)│   ├── search_Tool() 向量搜索│   └── get_tool() 工具详情└── API路由    ├── MCPClient (MCP集成)    └── DashScopeClient (大模型交互)
主要类描述1. CustomEmbeddingFunction 类(出问题找王闯师兄,我只调用API)
功能 : 处理向量嵌入请求[[RAG]]
作用 : 向嵌入服务发送请求，将文本转换为向量表示
关键方法 : call() - 处理嵌入请求并返回向量

2. Tool 类（数据库模型）
功能 : 定义工具数据表结构
作用 : 映射MySQL数据库中的tools表
包含字段 : id、name、description、parameters、host、method、url等工具配置信息

3. ToolCapability 类(元芳执行器的调用方式,元芳能用该工具,我就能用)
功能 : 工具能力管理核心类
作用 : 管理ChromaDB连接和工具搜索功能
关键方法 :
init() - 初始化数据库连接
search_Tool() - 基于任务描述搜索相关工具
get_tool() - 从数据库获取工具详情



4. MCPClient 类（在api_clients.py中）
功能 : MCP服务器客户端
作用 : 连接MCP服务器并处理工具调用
关键方法 : connect_to_server() , process_query()

5. DashScopeClient 类（在api_clients.py中）
功能 : 通义千文API客户端
作用 : 与大模型进行对话交互
关键方法 : chat() - 发送聊天请求

以后的元芳工具描述格式名称:[MCP]_[核心动作]_[目标对象]_[工具类型]描述:	功能说明：xxxxx	输入参数：	- xxx：xxxxxxx	适用场景：	- xxxxx	- xxxxx	- xxxxx例如:名称:[MCP]查询_虎符运行指标_Prometheus_服务描述:	功能说明：通过PromQL查询语句，获取获取虎符系统的各类运行指标数据。	输入参数：	- query：字符串，符合PromQL语法的Prometheus查询语句。	适用场景：	- 监控虎符系统运行状态	- 排查虎符服务异常	- 收集性能指标数据

工具的接入依靠1）先是向量检索工具数据库中的描述和名称字段，筛选出三个符合的描述；2）将这三个工具开放的接口连同用户的提问直接交给大模型，大模型会自动填充和调用工具，工具执行过程使用的是另一份元芳执行器的执行代码
Qwen的MCP格式final_response&#123;    &quot;status_code&quot;: 200,                    # HTTP状态码    &quot;code&quot;: &quot;Success&quot;,                     # 业务状态码    &quot;message&quot;: &quot;success&quot;,                  # 业务消息    &quot;output&quot;: &#123;                           # 主要输出内容        &quot;choices&quot;: [            &#123;                &quot;message&quot;: &#123;                    &quot;role&quot;: &quot;assistant&quot;,                    &quot;content&quot;: &quot;模型生成的最终回答内容&quot;                &#125;            &#125;        ]    &#125;,    &quot;usage&quot;: &#123;                            # 使用统计        &quot;input_tokens&quot;: 100,        &quot;output_tokens&quot;: 50,        &quot;total_tokens&quot;: 150    &#125;&#125;

项目意义由于MCP完美适配元芳的“应用-工具”模式, 如果使用MCP, 将不用在直接手动设定某个被调用的工具, MCP可以帮助我们选择工具, 并将参数自动填入, 最后将工具的结果变为自然语言, 我们就能很舒服的通过问问题而调用元芳的所有工具.
项目来历2024年11月,MCP的诞生[[MCP]]2025年4月,丁安然师姐介绍MCP专题2025年5月,我开始尝试将MCP接入元芳,一开始使用OpenAI的模型,最后改为使用Qwen的模型2025年7月,最后将元芳的工具描述补充完毕.2025年9月,放完暑假,修改部分bug后, 最终将MCP上传Githubyuanfang-mcp








项目进度一开始, 丁安然师姐在介绍MCP的时候直接通过API的调用接入了三个工具, 并手动添加了工具的描述,和元芳的关系不大, 只是作为演示.之后我便被徐老师安排将MCP接入元芳.
当时对元芳还不太熟悉,再加上代码能力不够,走了很多很多弯路, 写了很多垃圾代码. 最终确定了先画设计图,定义项目结构,最后再进行编程.![[ObsidianPicture&#x2F;Pasted image 20250930180205.png]]
这里附上我写的流程图(其中的问题已经全部解决了,只用关心流程)
![[ObsidianPicture&#x2F;Pasted image 20250930180147.png]]虽然,画了图,但是还是写了一坨屎山.主要原因还是不太熟悉元芳和虎符.
]]></content>
      <categories>
        <category>果冻的航海日志</category>
      </categories>
      <tags>
        <tag>LLM</tag>
        <tag>元芳</tag>
      </tags>
  </entry>
</search>
